<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>贾磊的猜想</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="记录编程的每一点">
<meta property="og:type" content="website">
<meta property="og:title" content="贾磊的猜想">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="贾磊的猜想">
<meta property="og:description" content="记录编程的每一点">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="贾磊的猜想">
<meta name="twitter:description" content="记录编程的每一点">
  
    <link rel="alternative" href="/atom.xml" title="贾磊的猜想" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.f6a68c.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

</head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="null" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">贾磊的猜想</a></h1>
		</hgroup>
		
		<p class="header-subtitle">爱生活，爱编程，爱晔子</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/tags/随笔/">博客</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="null" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">贾磊的猜想</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>爱生活，爱编程，爱晔子<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/tags/随笔/">博客</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-seventh-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/27/seventh-article/">小程序webview传递Cookie引发的思考</a>
    </h1>
  

        
        <a href="/2018/05/27/seventh-article/" class="archive-article-date">
  	<time datetime="2018-05-27T09:07:55.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-05-27</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="一-事发场景"><a href="#一-事发场景" class="headerlink" title="一. 事发场景"></a>一. 事发场景</h3><p>​      最近开发小程序，早就听说小程序开发了webview控件，一些静态H5页面可以直接考虑通过webview跳转url打开，开发过程中发现小程序的webview没有提供直接设置Cookie的方法，只能在url中带上服务器返回的session,  于是和后端的同学愉快的决定了，在url里带上cookie=’xxx’, 后端从cookie里解析出Cookie去做处理就可以了，看似天衣无缝，其实里面隐藏了一个很严重的理解问题，经过同事的指点，这里url带上的用户标识并不是Cookie，而是服务端的sessionId。下图是小程序的response headers，</p>
<p><img src="/2018/05/27/seventh-article/image-1.jpeg" alt="image-1"></p>
<p>​    可以看到，每次请求服务端都会通过Set-Cookie的标准形式设置ResponseHeaders的cookie，这里Cookie是一个key-value形式的值，除了session还有其他信息，所以cookie也经常被称为cookies，其中的服务器session只是通过Cookie来传递的一个值，所以前端再请求url中带上的应该是这个sessionId。</p>
<p>​    由于微信并没有给小程序开放保存，设置Cookie的接口，只能手动取出在请求中的Set-Cookie字段，解析出session，在发起请求的时候设置到请求header中。另外在浏览器中有一套成熟且自动的解析，保存，设置Cookie的机制。</p>
<h3 id="二-Cookie是什么"><a href="#二-Cookie是什么" class="headerlink" title="二. Cookie是什么"></a>二. Cookie是什么</h3><h6 id="1-Cookie的由来："><a href="#1-Cookie的由来：" class="headerlink" title="1.Cookie的由来："></a>1.Cookie的由来：</h6><p>​    Cookie 技术产生源于 HTTP 协议在互联网上的急速发展。随着互联网的深层次发展，人们需要更复杂的互联网交互活动，比如需要知道哪些人往自己的购物车中放了商品，  也就是说我必须把每个人区分开，记录每个访问者的状态。然而HTTP是一个“无状态”协议，为了在用户访问时知道当前用户的信息，1993 年，网景公司雇员 Lou Montulli 想到了可以把服务器生成的session传给浏览器保存起来，下次访问再带上这个session就可以了，于是发明了Cookie在里面设置和传递服务端session的id。</p>
<h6 id="2-cookie实现和特点："><a href="#2-cookie实现和特点：" class="headerlink" title="2.cookie实现和特点："></a>2.cookie实现和特点：</h6><p>​    Cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示浏览器按照指示生成相应的Cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的Cookie，如果某个Cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该Cookie附在请求资源的HTTP请求头上发送给服务器。</p>
<p>​    Cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成Cookie的作用范围。若不设置过期时间，则表示这个Cookie的生命期为浏览器会话期间，关闭浏览器窗口，Cookie就消失。这种生命期为浏览器会话期的Cookie被称为会话Cookie。会话Cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把Cookie保存到硬盘上，关闭后再次打开浏览器，这些Cookie仍然有效直到超过设定的过期时间。</p>
<h3 id="三-客户端Cookie实现"><a href="#三-客户端Cookie实现" class="headerlink" title="三. 客户端Cookie实现"></a>三. 客户端Cookie实现</h3><h6 id="1-iOS-cookie实现"><a href="#1-iOS-cookie实现" class="headerlink" title="1 iOS cookie实现"></a>1 iOS cookie实现</h6><p>​    ios提供了从URL中获取cookie和设置AFHTTPRequestSerializer cookie的方式，网上有大量的资料讲解，并且由于AFNetworking库使用NSURLSession和NSURLConnection实现，可以自动实现保存和设置Cookie，使用了AFNetworking库的项目完全不用操心cookie的实现。</p>
<p>​    当然可以在不需要cookies中的某些值时选择删除：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">for (NSHTTPCookie *cookie in [NSHTTPCookieStorage sharedHTTPCookieStorage].cookies) &#123;</div><div class="line">    if([cookie.name isEqualToString:@&quot;athena-session&quot;]) &#123;</div><div class="line">        [[NSHTTPCookieStorage sharedHTTPCookieStorage] deleteCookie:cookie];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<pre><code>并且AFNetworking保存Cookie后,通过UIWebView的loadRequest方法load NSURLRequest时可以共用cookie。
</code></pre><h6 id="2-WKWebview设置Cookie"><a href="#2-WKWebview设置Cookie" class="headerlink" title="2 WKWebview设置Cookie:"></a>2 WKWebview设置Cookie:</h6><p>​    之前提到UIWebView 中loadRequest可以自动使用保存好的 Cookie, 体会到了iOS框架设计者的贴心，然而事情不总是这么完美，iOS 8 后退出的WKWebview并没有这个能力，需要手动添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">- (void)loadRequest: (NSMutableURLRequest *)request &#123;</div><div class="line"></div><div class="line">    NSString * sessionId = @&quot;&quot;;</div><div class="line">    for (NSHTTPCookie * cookie in [NSHTTPCookieStorage sharedHTTPCookieStorage].cookies) &#123;</div><div class="line">        if ([cookie.name isEqualToString:@&quot;session&quot;]) &#123;</div><div class="line">            sessionId = cookie.value;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    NSString * session = [NSString stringWithFormat:@&quot;%@=%@&quot;,@&quot;session&quot;, sessionId];</div><div class="line"></div><div class="line">    [request addValue: sessionforHTTPHeaderField:@&quot;cookie&quot;];</div><div class="line"></div><div class="line">    [_webView loadRequest: request];</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/05/27/seventh-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
    <article id="post-sixth-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/07/sixth-article/">机器学习算法（四）— 支持向量机</a>
    </h1>
  

        
        <a href="/2018/02/07/sixth-article/" class="archive-article-date">
  	<time datetime="2018-02-07T15:33:05.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-07</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>​    支持向量机（Support Vector Machines, SVM）被许多人认为是现成的最好的分类器，“现成”指分类器不加修饰即可以直接使用。同时，这就意味着在数据上应用基本形式的SVM分类器就可以得到低错误率的结果。SVM能够对训练集之外的数据点做出很好的分类决策。下面介绍一种最流行的SVM实现，<strong>序列最小化算法（SMO）</strong></p>
<h4 id="一-基于最大间隔分割数据"><a href="#一-基于最大间隔分割数据" class="headerlink" title="一  基于最大间隔分割数据"></a>一  基于最大间隔分割数据</h4><ul>
<li><p>​    优点：泛化错误率低，计算开销不大，结果易于理解<br>   ​    缺点：对参数调节和核函数的选择敏感，原始分类器不修改仅适用于处理二类问题。<br>   ​    适用数据类型：数值型和标称型数据。        </p>
<p>  ​    </p>
<pre><code>在下图中，很容易在途中画出一条直线将两组数据点分开。在这种情况下，这组数据被称为线性可分数据，这条将数据分开的直线称为**分割超平面（separating hyperplane）**。
</code></pre></li>
</ul>
<p><img src="/Users/leijia/workspace/jialeigd/source/_posts/sixth-article/sixth_1.jpeg" alt="sixth_1"></p>
<p>​    在上面给出的例子中，由于数据点都在二维平面上，所以此时分隔超平面就只是一条直线。但是，如果所给的数据集是三维的，那么此时用来分隔数据的就是一个平面。显而易见，更高维的情况可以依此类推。如果数据集是1024维的，那么就需要一个1023维的某某对象来对数据进行分隔。这个1023维的某某对象到底应该叫什么?N-1维呢?该对象被称为<strong>超平面(hyperplane)</strong> ，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。</p>
<p>​    我们希望能采用这种方式来构建分类器，即如果数据点离决策边界越远,’那么其最后的预测结果也就越可信。考虑到上图中B到D的三条直线，他们都能将数据分隔开，但是哪一条最好呢？是否应该最小化数据点到分割超平面的平均距离？我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为<strong>间隔（margin）</strong>。我们希望间隔尽可能的大，这是如果我们犯错或者再有限数据上训练分类器的话，我们希望分类器尽可能的健壮。    </p>
<p>​    <strong>支持向量（support vector）</strong>就是离分割超平面最近的那些点。接下来要试着最大化支持向量到分割面的距离，需要找到此问题的优化求解方法。</p>
<h4 id="二-寻找最大间隔"><a href="#二-寻找最大间隔" class="headerlink" title="二 寻找最大间隔"></a>二 寻找最大间隔</h4><p>​    如何求解数据集的最佳分割直线？如下图，分割超平面的形式可以写成|Wª * A + b    | / ||w||.这里的向量W和常数b一起描述了所给数据的分隔线或超平面。</p>
<h5 id="1-分类器求解的优化问题"><a href="#1-分类器求解的优化问题" class="headerlink" title="1 分类器求解的优化问题"></a>1 分类器求解的优化问题</h5><p>​    前面已经提到分类器，但还没有介绍它的工作原理。理解其工作原理有助于理解基于优化问题的分类器求解过程，输入数据给分类器会输出一个类别标签，这相当于一个类似于sigmoid的函数在作用。下面使用单位阶跃函数的函数对WtX+b作用得到 f(WªX + b), 其中当u&lt;0时 输出-1，反之输出1.这里使用-1和1因为-1和1相差一个符号，方便数学上的处理。我们可以通过一个统一的公式来表示间隔或者数据点到分隔超平面的距离，同时不必担心数据到底是属于-1还是+1类。</p>
<p>​    当计算数据点到分隔面的距离并确定分割面的放置位置时，间隔通过label * (Wt + b) 来计算，这时就能体现出-1和1的好处。</p>
<p>​    现在的目标就是找出分类器定义中的w和b。为此，我们必须找到具有最小间隔的数据点，而这些数据点也就是前面提到的支持向量。一旦找到最小间隔的数据点，我们就需要对该间隔最大化。<br>$$<br>argmax{min(label.(Wtx + b)).1 / ||w||}<br>$$<br>​    直接求解上述问题相当困难，所以我们将它转换成为另一种更容易求解的形式。首先考察一下上式中大括号内的部分。由于对乘积进行优化是一件很讨厌的事情，因此我们要做的是固定其中一个因子而最大化其他因子。如果令所有支持向量的label.(Wtx + b) 都为1，那么就可以通过求1 / ||w||的最大值来得到最终解。但是，并非所有数据点的label.(Wtx + b)都等于1，只有那些离分隔超平面最近的点得到的值才为1。而离超平面越远的数据点，其label.(Wtx + b)的值也就越大。</p>
<p>​    在上述问题中，给定了一些约束条件然后求最优值，因此该问题是一个带约束条件的优化问题。这里的优化条件就是label.(Wtx + b).对于这类优化问题，有一个非常著名的求即拉格朗日乘子法。通过引人拉格朗日乘子，我们就可以基于约束条件来表述原来的问题。由于这里的约束条件都是基于数据点的，因此我们就可以将超平面写成数据点的形式。于是优化目标函数最后可以写成:</p>
<p>​    <img src="/Users/leijia/workspace/jialeigd/source/_posts/sixth-article/sixth_2.jpeg" alt="sixth_2"></p>
<p>​    其约束条件为</p>
<p><img src="/Users/leijia/workspace/jialeigd/source/_posts/sixth-article/sixth_3.jpeg" alt="sixth_3"></p>
<p>​    至此，一切都很完美，但是这里有个假设:数据必须线性可分。目前为止，我们知道几乎所有数据都不那么®干净 。这时我们就可以通过引人所谓松弛变量，来允许有些数据点可以处于分隔面的错误一侧。这样我们的优化目标就能保持仍然不变，但是此时新的约束条件则变为</p>
<p>​        <img src="/Users/leijia/workspace/jialeigd/source/_posts/sixth-article/sixth_4.jpeg" alt="sixth_4"><br>​    这里的常数C. 用于控制最大化间隔 和 保证大部分点的函数间隔小于1.0  这两个目标的权重。在优化算法的实现代码中，常数C是一个参数，因此我们就可以通过调节该参数得到不同的结果。一旦求出了所有的alpha，那么分隔超平面就可以通过这些alpha来表达。</p>
<p>​    </p>
<h5 id="2-SVM的一般框架"><a href="#2-SVM的一般框架" class="headerlink" title="2 SVM的一般框架"></a>2 SVM的一般框架</h5><ul>
<li>收集数据</li>
<li>准备数据</li>
<li>分析数据：有助于可视化分割超平面</li>
<li>训练算法：SVM大部分时间都源自训练，该过程主要是两个参数的调优</li>
<li>测试算法：十分简单的j计算过程就可以实现</li>
<li>使用算法：几乎所有分类问题都可以使用SVM。</li>
</ul>
<p>下面介绍一种SVM算法的实现。SMO算法</p>
<h4 id="三-SMO高效优化算法"><a href="#三-SMO高效优化算法" class="headerlink" title="三 SMO高效优化算法"></a>三 SMO高效优化算法</h4><p>​    接下来，我们根据6.2.1节中的最后两个式子进行优化，其中一个是最小化的目标函数，一个是在优化过程中必须遵循的约束条件。下面我们就来讨论SMO算法，</p>
<h5 id="1-platt的SMO算法"><a href="#1-platt的SMO算法" class="headerlink" title="1 platt的SMO算法"></a>1 platt的SMO算法</h5><p>​    1996年，platt发布了一个SMO的强大算法，用于训练SVM。SMO用于表示序列最小化。SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对他们进行顺序求解的结果与将他们整体求解的结果是一致的。在结果完全相同时，SMO算法求解时间短很多。</p>
<p>​    SMO算法的目的是求出一系列的alpha和B ,一旦求出了这些alpha，就很容易计算出权重向量W并得到分割超平面。</p>
<p>​    SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理，一旦找到一对合适的alpha，那么就增大其中一个减小另一个。这里的合适是指两个alpha要符合一定的条件，一是这两个alpha要在间隔边界之外，二是这两个alpha还没有进行过区间化处理或者不在边界上。</p>
<h5 id="2-应用简化版SMO算法处理小规模数据集："><a href="#2-应用简化版SMO算法处理小规模数据集：" class="headerlink" title="2 应用简化版SMO算法处理小规模数据集："></a>2 应用简化版SMO算法处理小规模数据集：</h5><p>​    下面我们会对算法进行简化处理，以便了解算法的基本工作思路，之后再给出完整版。首先在数据集上遍历每一个alpha，然后再剩下alpha集合中随机选择另外一个alpha来构建alpha对。有一点相当重要，就是要同时改变两个alpha对。之所以这样做是因为有一个约束条件：<br>$$<br>∑å•label(i) = 0<br>$$<br>由于改变一个alphah会导致该约束条件失败，因此我们总是同时改变两个alpha。</p>
<p>​    下面构建两个辅助函数一个负责在某个区间随机选择一个整数，另一个负责用于在数值太大时对其进行调整。下面给出实现。</p>
<p>​    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">def loadDataSet(fileName):</div><div class="line">    dataMat = []</div><div class="line">    labelMat = []</div><div class="line"></div><div class="line">    with open(fileName) as file:</div><div class="line">        for line in file.readlines():</div><div class="line">            lineArr = line.strip().split(&apos;\t&apos;)</div><div class="line">            dataMat.append([float(lineArr[0]), float(lineArr[1])])</div><div class="line">            labelMat.append(float(lineArr[2]))</div><div class="line"></div><div class="line">    return dataMat, labelMat</div><div class="line"></div><div class="line"></div><div class="line">def selectedJRand(i, m):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    只要函数值不等于输入值i，函数就会进行随机选择</div><div class="line">    :param i: 第一个alpha的下标</div><div class="line">    :param m: alpha的数目</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    j = i</div><div class="line">    while(j == i):</div><div class="line">        j = int(random.uniform(0, m))</div><div class="line"></div><div class="line">    return j</div><div class="line"></div><div class="line"></div><div class="line">def clipAlpha(aj, H, L):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    用于调整大于H和小于L的值</div><div class="line">    :param aj: </div><div class="line">    :param H: </div><div class="line">    :param L: </div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    if aj &gt; H:</div><div class="line">        aj = H</div><div class="line">    if aj &lt; L:</div><div class="line">        aj = L</div><div class="line"></div><div class="line">    return aj</div></pre></td></tr></table></figure>
<p>函数selectedJRand有两个参数值，其中i是第一个alpha的下标，m是所有alpha的数目。只要函数值不等于输人值i，函数就会进行随机选择。    最后一个辅助函数就是clipAlpha，它是用于调整大于H或小于L的alpha值 。尽管上述 三个辅助函数本身做的事情不多，但在分类器中却很有用处。    </p>
<p>​    SMO算法的伪代码大致如下：</p>
<p>创建一个alpha向量并将其初始化为0向量</p>
<p>当迭代次数小于最大迭代次数时（外循环）</p>
<p>​    对数据集中的每个数据向量（内循环）：</p>
<p>​        如果该数据向量可以被优化：</p>
<p>​            随机选择另外一个数据向量</p>
<p>​            同时优化这两个向量</p>
<p>​            如果两个向量都不能被优化，退出内循环</p>
<p>​    如果所有向量都没被优化，增加迭代数目，继续下一次循环</p>
<p>​    简化版SMO算法：</p>
<p>​    构建Svm分类器类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import dataReader as dr</div><div class="line"></div><div class="line">class SvmClassifier(object):</div><div class="line">    def __init__(self, C=0.5, error=0.001, iteration=10):</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        分类器初始化方法</div><div class="line">        :param C:   松弛系数</div><div class="line">        :param error:   容错率</div><div class="line">        :param iteration:   迭代次数</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        self.C = C</div><div class="line">        self.error = error</div><div class="line">        self.iteration = iteration</div><div class="line"></div><div class="line">        self.alphas = None</div><div class="line">        self.b = 0</div></pre></td></tr></table></figure>
<p>​            </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">def train(self, trainData, labels):</div><div class="line">    trainDataMat = np.mat(trainData)</div><div class="line">    labelsDataMat = np.mat(labels).transpose()</div><div class="line"></div><div class="line">    m, n = trainDataMat.shape</div><div class="line">    alphas = np.mat(np.zeros((m, 1)))</div><div class="line">    b = 0</div><div class="line"></div><div class="line">    currentIter = 0</div><div class="line">    while currentIter &lt; self.iteration:</div><div class="line">        alphaChanged = False</div><div class="line"></div><div class="line">        for i in range(m):</div><div class="line">            ui = SvmClassifier.fx(trainDataMat, labelsDataMat, alphas, b, i)</div><div class="line">            errorI = ui - float(labelsDataMat[i])</div><div class="line"></div><div class="line">            if ((labelsDataMat[i] * errorI &lt; -self.error) and (alphas[i] &lt; self.C) or\</div><div class="line">                        ((labelsDataMat[i] * errorI &gt; self.error) and (alphas[i] &gt; 0))):</div><div class="line">                j = dr.selectedJRand(i, m)</div><div class="line">                uj = SvmClassifier.fx(trainDataMat, labelsDataMat, alphas, b, j)</div><div class="line">                errorJ = uj - float(labelsDataMat[j])</div><div class="line"></div><div class="line">                oldAlphaI = alphas[i].copy()</div><div class="line">                oldAlphaJ = alphas[j].copy()</div><div class="line"></div><div class="line">                if labelsDataMat[i] != labelsDataMat[j]:</div><div class="line">                    L = max(0., alphas[j] - alphas[i])</div><div class="line">                    H = min(self.C, self.C + alphas[j] - alphas[i])</div><div class="line">                else:</div><div class="line">                    L = max(0., alphas[j] + alphas[i] - self.C)</div><div class="line">                    H = min(self.C, alphas[j] + alphas[i])</div><div class="line"></div><div class="line">                if L == H:</div><div class="line">                    continue</div><div class="line"></div><div class="line">                eta = trainDataMat[i, :] * trainDataMat[i, :].T + trainDataMat[j, :] * trainDataMat[j, :].T +\</div><div class="line">                      2.0 * trainDataMat[i, :] * trainDataMat[j, :].T</div><div class="line"></div><div class="line">                if eta &lt; 0:</div><div class="line">                    continue</div><div class="line"></div><div class="line">                alphas[j] += labelsDataMat[j] * (errorI - errorJ) / eta</div><div class="line">                alphas[j] = dr.clipAlpha(alphas[j], L, H)</div><div class="line"></div><div class="line">                if abs(alphas[j] - oldAlphaJ) &lt; 0.00001:</div><div class="line">                    continue</div><div class="line"></div><div class="line">                alphas[i] += labelsDataMat[j] * labelsDataMat[i] * (oldAlphaJ - alphas[j])</div><div class="line">                b1 = b - errorI - labelsDataMat[i] * (alphas[i] - oldAlphaI) * trainDataMat[i, :] * trainDataMat[i, :].T - \</div><div class="line">                     labelsDataMat[j] * (alphas[j] - oldAlphaJ) * trainDataMat[i, :] * trainDataMat[j, :].T</div><div class="line"></div><div class="line">                b2 = b - errorJ - labelsDataMat[i] * (alphas[i] - oldAlphaI) * trainDataMat[i, :] * trainDataMat[j, :].T - \</div><div class="line">                     labelsDataMat[j] * (alphas[j] - oldAlphaJ) * trainDataMat[j, :] * trainDataMat[j, :].T</div><div class="line"></div><div class="line">                if 0 &lt; alphas[i] &lt; self.C:</div><div class="line">                    b = b1</div><div class="line">                elif 0 &lt; alphas[j] &lt; self.C:</div><div class="line">                    b = b2</div><div class="line">                else:</div><div class="line">                    b = (b1 + b2) / 2</div><div class="line"></div><div class="line">                print(&apos;Iteration: &#123;iteration&#125;, i : &#123;i&#125; changed&apos;.format(self.iteration))</div><div class="line"></div><div class="line">        if not alphaChanged:</div><div class="line">            currentIter += 1</div><div class="line">        else:</div><div class="line">            currentIter = 0</div><div class="line"></div><div class="line">    self.alphas = alphas</div><div class="line">    self.b = b</div></pre></td></tr></table></figure>
<p>​    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">@staticmethod</div><div class="line">def fx(trainMat, labelMat, alphas, b, i):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    根据目标函数来预测数据</div><div class="line">    :param trainMat:    训练数据</div><div class="line">    :param labelMat:    标签数据</div><div class="line">    :param alphas:      </div><div class="line">    :param b:   </div><div class="line">    :param i:           参数索引</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    return float(np.multiply(alphas, labelMat).T * (trainMat * trainMat[i, :].T)) + b</div></pre></td></tr></table></figure>
<p>​    </p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/07/sixth-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
    <article id="post-fifth-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/08/fifth-article/">机器学习算法之（三）—基于概率论的分类方法：朴素贝叶斯</a>
    </h1>
  

        
        <a href="/2017/10/08/fifth-article/" class="archive-article-date">
  	<time datetime="2017-10-08T01:29:09.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-10-08</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>####一  基于贝叶斯决策理论的分类方法</p>
<h5 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h5><ul>
<li>优点：在数据少的情况下依然有效，可以处理多类别问题</li>
<li>缺点：对于输入数据的准备方式较为敏感</li>
<li>使用数据类型：标称型数据</li>
</ul>
<p>​        朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素负叶斯之前有必要快速了解一下贝叶斯决策理论。假设现在我们有一个数据集，它由两类数据组成，假设有位读者找到了描述图中两类数据的统计参数。我们使用p1(x,y)表示数据点（x,y）属于1的概率，p2(x,y)表示属于类别2的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别:</p>
<p>​    如果p1(x,y) &gt; p2(x,y) 那么类别为1</p>
<p>​    如果p1(x,y) &lt; p2(x,y) 那么类别为2</p>
<p>​    也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。如果整个数据集使用6个浮点数来表示，并且计算类别概率的Python代码只有两行，那么你更倾向于那种方法来对该数据点进行分类？</p>
<ol>
<li><p>使用knn，进行1000次距离计算</p>
</li>
<li><p>使用决策树，分别沿着x轴，y轴划分数据集</p>
</li>
<li><p>计算数据点属于每个类别的概率，并进行比较</p>
<p>使用决策树不会非常成功, 而和简单的概率计算相比, kNN 的计算量太大。因此，对于上述问题，最佳选择是使用刚才提到的概率比较方法。    </p>
</li>
</ol>
<h5 id="2-使用条件概率来进行分类"><a href="#2-使用条件概率来进行分类" class="headerlink" title="2 使用条件概率来进行分类"></a>2 使用条件概率来进行分类</h5><p>​    上节提到的两个概率，P1和P2：</p>
<p>​    如果p1(x,y) &gt; p2(x,y) 那么类别为1</p>
<p>​    如果p1(x,y) &lt; p2(x,y) 那么类别为2</p>
<p>​    但这两个准则并不是贝叶斯决策理论的所有内容。使用P1和P2只是尽可能的简化描述，而真正需要计算和比较的是P(c1|x, y)和P(c2 | x, y)。这些符号所代表的具体意义是:给定某个由x,y表示的数据点，那么该数据点来自类别c1,c2的概率分别是多少? 具体地，应用贝叶斯准则得到：<br>$$<br>p(ci |x, y) = p(x,y|ci)p(ci) / p(x, y)<br>$$</p>
<h4 id="二-使用朴素贝叶斯对文档进行分类"><a href="#二-使用朴素贝叶斯对文档进行分类" class="headerlink" title="二 使用朴素贝叶斯对文档进行分类"></a>二 使用朴素贝叶斯对文档进行分类</h4><p>​    机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档(如一封电子邮件)是实例，而电子邮件中的某些元素则构成特征。虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。</p>
<h5 id="朴素贝叶斯的一般过程："><a href="#朴素贝叶斯的一般过程：" class="headerlink" title="朴素贝叶斯的一般过程："></a>朴素贝叶斯的一般过程：</h5><ol>
<li>数据收集:可以使用任何方法。本章使用RSS源。    </li>
<li>准备数据:需要数值型或者布尔型数据。</li>
<li>分析数据:有大量特征时，绘制特征作用不大，此时使用直方图效果更好。<ol>
<li>算法:计算不同的独立特征的条件概率。                                    <ol>
<li>测试算法:计算错误率。    </li>
</ol>
</li>
</ol>
</li>
<li>使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴</li>
<li><p>素贝叶斯命类器，不一定非要是文本</p>
<p>​        </p>
<p>​假设词汇表中有1000个单词。要得到好的概率分布，就需要足够的数据样本，假定样本数为N。由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要N^10个样本，对于包含1000个特征的词汇表将需要个样本。如果特征之间相互独立，那么样本数就可以从N^1000减少到1000 * N.所谓独立是指统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，朴素贝叶斯的另一个假设是，每个特征同等重要。<br>其实这个假设也有问题。如果要判断留言板的留言是否得当，那么可能不需要看完所有的1000个单词，而只需要看10~20个特征就足以做出判断了。尽管上述假设存在一些小的瑕 疵 ，但朴素贝叶斯的实际效果却很好。    </p>
</li>
</ol>
<p>​    </p>
<h4 id="三-使用Python进行文本分类"><a href="#三-使用Python进行文本分类" class="headerlink" title="三 使用Python进行文本分类"></a>三 使用Python进行文本分类</h4><p>​    要从文本中获取特征，需要先拆分文本。具体如何做呢，这里的特征是来自文本的词条(token), 一个词条是字符的任意组合。可以把词条想象为单词，也可以使用非单词词条，如URL，IP地址或者任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。</p>
<h5 id="1-准备数据-从文本中构建词向量"><a href="#1-准备数据-从文本中构建词向量" class="headerlink" title="1 准备数据:从文本中构建词向量"></a>1 准备数据:从文本中构建词向量</h5><p>​    我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳人词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">def loadDataSet():</div><div class="line">    postingList = [[&apos;my&apos;, &apos;dog&apos;, &apos;has&apos;, &apos;flea&apos;, &apos;problems&apos;, &apos;help&apos;, &apos;please&apos;],</div><div class="line">                  [&apos;maybe&apos;, &apos;not&apos;, &apos;take&apos;, &apos;him&apos;, &apos;to1&apos;, &apos;dog&apos;, &apos;park&apos;, &apos;stupid&apos;],</div><div class="line">                  [&apos;my&apos;, &apos;dalmation&apos;, &apos;is&apos;, &apos;so&apos;, &apos;cute&apos;, &apos;my&apos;, &apos;love&apos;, &apos;him&apos;],</div><div class="line">                  [&apos;stop&apos;, &apos;posting&apos;, &apos;stupid&apos;, &apos;worthless&apos;, &apos;garbage&apos;],</div><div class="line">                  [&apos;mr&apos;, &apos;licks&apos;, &apos;ate&apos;, &apos;my&apos;, &apos;steak&apos;, &apos;how&apos;,&apos;to&apos;, &apos;stop&apos;, &apos;him&apos;]</div><div class="line">                  [&apos;quit&apos;, &apos;buying&apos;, &apos;worthless&apos;, &apos;dog&apos; &apos;food&apos;, &apos;stupid&apos;]]</div><div class="line"></div><div class="line">    classVec = [0, 1, 0, 1, 0, 1]</div><div class="line"></div><div class="line">    return postinglist, class_vec</div><div class="line"></div><div class="line"></div><div class="line">def create_vocablist(dataSet):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    创建一个包含在所有文档中出现的不重复词的列表</div><div class="line">    :param dataSet: </div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    vocab_set = set([])</div><div class="line">    for document in dataSet:</div><div class="line">        vocab_set = vocab_set | set(document)</div><div class="line"></div><div class="line">    return list(vocab_set)</div><div class="line"></div><div class="line">def set_of_words_to_vec(vocab_list, input_set):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    检查input_set 文档向量，如果出现了vocab_list 中的单词，将input_set 对应元素置为1.</div><div class="line">    :param vocab_list:      词汇表</div><div class="line">    :param input_set:       输入的检查词汇集合</div><div class="line">    :return:    标志位文档向量</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    return_vec = [0] * len(vocab_list)</div><div class="line">    for word in input_set:</div><div class="line">        if word in vocab_list:</div><div class="line">            return_vec[vocab_list.index(word)] = 1</div><div class="line">        else:</div><div class="line">            print (&quot;the word :%s is not in my Vocabulary&quot; % word)</div><div class="line"></div><div class="line">    return return_vec</div></pre></td></tr></table></figure>
<p>​        </p>
<h5 id="2-训练算法：从词向量计算概率"><a href="#2-训练算法：从词向量计算概率" class="headerlink" title="2 训练算法：从词向量计算概率"></a>2 训练算法：从词向量计算概率</h5><p>​    前面介绍了如何将一组单词转换为一组数字，接下来看看如何使用这些数字计算概率。现在已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。我们重写贝叶斯准则。w表示这是一个向量，即他有多个数值组成。在这个例子中，数值个数与词汇表中的词个数相同<br>$$<br>p (Ci| w) = p (w | Ci) * p(Ci) / p(w)<br>$$<br>​    我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。如何计算呢首先可以通过类别I(侮辱性留言或非侮辱性留言)中文档数除以总的文档数来计算概率P(Ci),接下来要计算p(w | Ci), 这里就要用到朴素贝叶斯假设。如果将p(w | Ci)展开为一个个独立特征，那么就可以将上述概率写作p(W0, W1… Wn | Ci)。</p>
<p>该函数的伪代码如下:</p>
<blockquote>
<p>计算每个类别中的文档数目:</p>
<p>对每篇训练文档:</p>
<p>​    对每个类别:</p>
<p>​        如果词条出现文档中―增加该词条的计数值</p>
<p>​        增加所有词条的计数值</p>
<p>​    对每个类别:</p>
<p>​        对每个词条:</p>
<p>​            将该词条的数目除以总词条数目得到条件概率</p>
<p>​    返回每个类别的条件概率    </p>
</blockquote>
<p>我们利用下面的代码来实现上述伪码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">def train_bayes(train_matrix, traion_category):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    </div><div class="line">    :param train_matrix:    文档矩阵</div><div class="line">    :param traion_category: 每篇文档类别标签所构成的向量</div><div class="line">    :return: plVect（侮辱性词向量概率分布）plVect（文明性词向量概率分布）</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    num_train_doc = len(train_matrix)</div><div class="line">    num_words = len(train_matrix[0])</div><div class="line"></div><div class="line">    p_abusive = np.sum(traion_category) / float(num_train_doc)</div><div class="line">    p0_number = np.ones(num_words)	//初始化为1</div><div class="line">    p1_number = np.ones(num_words)</div><div class="line">    p0_denom = 2.0</div><div class="line">    p1_denom = 2.0</div><div class="line"></div><div class="line">    for i in range(num_train_doc):</div><div class="line">        # 计算侮辱性词的权重和总数</div><div class="line">        if traion_category[i] == 1:</div><div class="line">            p1_number += train_matrix[i]</div><div class="line">            p1_denom += np.sum(train_matrix[i])</div><div class="line">        # 计算侮辱性词的权重和总数</div><div class="line">        else:</div><div class="line">            p0_number += train_matrix[i]</div><div class="line">            p0_denom += np.sum(train_matrix[i])</div><div class="line"></div><div class="line">    plVect = p1_number / p1_denom</div><div class="line">    p0Vect = p0_number / p0_denom</div><div class="line">    </div><div class="line">    return p0Vect, plVect, p_abusive</div></pre></td></tr></table></figure>
<p>​    利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率 ，即计算        p(W0|1)p(W1|1)p(W2|1) ,如果其中一个概率值为0,那么最后的乘积也为0。为降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。</p>
<p>​    下面利用以上方法来计算侮辱性文档的概率和两个类别的概率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    list_posts, list_classes = reader.loadDataSet()</div><div class="line">    my_vocab_list = reader.create_vocablist(list_posts)</div><div class="line"></div><div class="line">    print (&apos;word list is %s&apos; % my_vocab_list)</div><div class="line"></div><div class="line">    train_matrix = []</div><div class="line">    for post_in_doc in list_posts:</div><div class="line">        train_matrix.append(reader.set_of_words_to_vec(my_vocab_list, post_in_doc))</div><div class="line"></div><div class="line">    p0_vec, p1_vec, p_ab = train_bayes(train_matrix, list_classes)</div><div class="line"></div><div class="line">    print (&apos;文明词概率 %s, \n侮辱词概率 %s&apos; % (p0_vec, p1_vec))</div><div class="line"></div><div class="line">    most_civilization_index = np.argmax(p0_vec)</div><div class="line">    most_not_civilization_index = np.argmax(p1_vec)</div><div class="line"></div><div class="line">    most_civilization = my_vocab_list[most_civilization_index]</div><div class="line">    most_not_civilization = my_vocab_list[most_not_civilization_index]</div><div class="line"></div><div class="line">    print (&apos;最文明词 %s, 最侮辱词 %s&apos; % (most_civilization, most_not_civilization))</div></pre></td></tr></table></figure>
<h5 id="3-测试算法-根据现实情况修改分类器"><a href="#3-测试算法-根据现实情况修改分类器" class="headerlink" title="3 测试算法:根据现实情况修改分类器"></a>3 测试算法:根据现实情况修改分类器</h5><p>​    上述算法会遇到下溢出的问题，这 是 由 于 太 多 很 小 的 数 相 乘 造 成 的 。 当计算乘积    p(W0|Ci)p(W1|Ci)p(W2|Ci), 由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。一种解决方法是对乘机取自然对数。    在代数中有ln(a * b) = ln(a)+ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plVect = log(p1_number / p1_denom)</div><div class="line"></div><div class="line">p0Vect = log(p0_number / p0_denom)</div></pre></td></tr></table></figure>
<p>​    现在已经构建好了完整的分类器了，下面来进行分类    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">def classify_naive_bayes(vec_classify, p0_vec, p1_vec, p_class):</div><div class="line">	//使用NumPy的数组来计算两个向量相乘的结果</div><div class="line">    p1 = np.sum(vec_classify * p1_vec) + np.log(p_class)</div><div class="line">    p0 = np.sum(vec_classify * p0_vec) + np.log(p_class)</div><div class="line">    </div><div class="line">    //比较类别的概率返回大概率 对应的类别标签</div><div class="line">    if p1 &gt; p0:</div><div class="line">        return &quot;侮辱&quot;</div><div class="line">    else:</div><div class="line">        return &quot;文明&quot;</div><div class="line"></div><div class="line">def testing_naive_bayes():</div><div class="line">    list_posts, list_classes = reader.loadDataSet()</div><div class="line">    my_vocab_list = reader.create_vocablist(list_posts)</div><div class="line"></div><div class="line">    print (&apos;word list is %s&apos; % my_vocab_list)</div><div class="line"></div><div class="line">    train_matrix = []</div><div class="line">    for post_in_doc in list_posts:</div><div class="line">        train_matrix.append(reader.set_of_words_to_vec(my_vocab_list, post_in_doc))</div><div class="line"></div><div class="line">    p0_vec, p1_vec, p_ab = train_bayes(train_matrix, list_classes)</div><div class="line">    print (&apos;文明词概率 %s, \n侮辱词概率 %s&apos; % (p0_vec, p1_vec))</div><div class="line"></div><div class="line">    most_civilization_index = np.argmax(p0_vec)</div><div class="line">    most_not_civilization_index = np.argmax(p1_vec)</div><div class="line"></div><div class="line"></div><div class="line">    most_civilization = my_vocab_list[most_civilization_index]</div><div class="line">    most_not_civilization = my_vocab_list[most_not_civilization_index]</div><div class="line"></div><div class="line">    print (&apos;最文明词 %s, 最侮辱词 %s&apos; % (most_civilization, most_not_civilization))</div><div class="line"></div><div class="line">    testEntry = [&apos;stupid&apos;, &apos;love&apos;, &apos;my&apos;, &apos;dog&apos;]</div><div class="line">    this_doc = np.array(reader.set_of_words_to_vec(my_vocab_list, testEntry))</div><div class="line">    type = classify_naive_bayes(this_doc, p0_vec, p1_vec, p_ab)</div><div class="line"></div><div class="line">    print (&apos;%s classified is %s&apos; % (testEntry, type))</div></pre></td></tr></table></figure>
<p>​        </p>
<h5 id="4-准备数据-文档词袋模型"><a href="#4-准备数据-文档词袋模型" class="headerlink" title="4 准备数据:文档词袋模型"></a>4 准备数据:文档词袋模型</h5><p>​    目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为词集模型(set-of-words-model)。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为词袋模型化(bag-of-words-model).    在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数set_of_words_to_vec（）稍加修改，修改后的函数称为    bag_of_words_to_vec</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def bag_of_words_to_vec(vocab_list, input_set):</div><div class="line">    return_vec = [0] * len(vocab_list)</div><div class="line">    for word in input_set:</div><div class="line">        if word in vocab_list:</div><div class="line">            return_vec[vocab_list.index(word)] += 1</div><div class="line"></div><div class="line">    return return_vec</div></pre></td></tr></table></figure>
<h4 id="四-示例-使用朴素贝叶斯分类器从订阅新闻中获取新闻类型"><a href="#四-示例-使用朴素贝叶斯分类器从订阅新闻中获取新闻类型" class="headerlink" title="四 示例:使用朴素贝叶斯分类器从订阅新闻中获取新闻类型"></a>四 示例:使用朴素贝叶斯分类器从订阅新闻中获取新闻类型</h4><p>​    在这个例子中，我们将分别从两个新闻订阅源选取一些新闻，通过这些新闻，来比较两类订阅源内容的不同。            </p>
<h5 id="示例-使用朴素贝叶斯来发现地域相关的用词"><a href="#示例-使用朴素贝叶斯来发现地域相关的用词" class="headerlink" title="示例:使用朴素贝叶斯来发现地域相关的用词"></a>示例:使用朴素贝叶斯来发现地域相关的用词</h5><p>收集数据：从RSS源收集内容，这里需要对RSS源构建一个接口    </p>
<p>准备数据：将文本文件解析成词条向量</p>
<p>分析数据：检查词条确保解析的正确性</p>
<p>训练算法：使用之前建立的train_bayes函数</p>
<p>测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。</p>
<p>使用算法:  构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。            </p>
<h5 id="1-收集数据-导入RSS源"><a href="#1-收集数据-导入RSS源" class="headerlink" title="1 收集数据:导入RSS源"></a>1 收集数据:导入RSS源</h5><p>​    接下来要做的第一件事是使用python下载文本。幸好，利用RSS，这些文本很容易得到。现在所需要的是一个RSS阅读器。Universal Feed Parse 中最常用的RSS程序库。可以构建一个类似于spamTest()的函数来对测试过程自动化。</p>
<p>RSS 源分类器及高频词去除函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line">def calc_most_freq(vocal_list, full_text):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    遍历词汇表vocal_list中每个词并统计它在文本中出现的次数，</div><div class="line">    然后根据出现的次数从高到低对词典进行排序，最后返回排序最高的100个词</div><div class="line">    :param vocal_list:  词汇表</div><div class="line">    :param full_text:   全部文本</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    freq_dic = &#123;&#125;</div><div class="line">    for token in vocal_list:</div><div class="line">        freq_dic[token] = full_text.count(token)</div><div class="line"></div><div class="line">    sorted_freq = sorted(freq_dic.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"></div><div class="line">    return sorted_freq[:30]</div><div class="line"></div><div class="line">def local_words(feed0, feed1):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    </div><div class="line">    :param feed1:   RSS源1</div><div class="line">    :param feed0:   RSS源2</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    doc_list = []</div><div class="line">    class_list = []</div><div class="line">    full_text = []</div><div class="line"></div><div class="line">    min_len = min(len(feed0[&apos;entries&apos;]), len(feed1[&apos;entries&apos;]))</div><div class="line"></div><div class="line">    for index in range(min_len):</div><div class="line">        word_list = data_reader.text_parse(feed0[&apos;entries&apos;][index][&apos;summary&apos;])</div><div class="line">        doc_list.append(word_list)</div><div class="line">        full_text.extend(word_list)</div><div class="line">        class_list.append(0)</div><div class="line"></div><div class="line">        word_list = data_reader.text_parse(feed1[&apos;entries&apos;][index][&apos;summary&apos;])</div><div class="line">        doc_list.append(word_list)</div><div class="line">        full_text.extend(word_list)</div><div class="line">        class_list.append(1)</div><div class="line"></div><div class="line">    vocal_list = data_reader.create_vocablist(doc_list)</div><div class="line"></div><div class="line">    top_30_words = calc_most_freq(vocal_list, full_text)</div><div class="line"></div><div class="line">    for pair_w in top_30_words:</div><div class="line">        if pair_w[0] in vocal_list:</div><div class="line">            vocal_list.remove(pair_w[0])</div><div class="line"></div><div class="line">    traing_set = list(range(2 * min_len))</div><div class="line">    test_set = []</div><div class="line"></div><div class="line">    for index in range(len(traing_set)):</div><div class="line">        randIndex = int(random.uniform(0, len(traing_set)))</div><div class="line">        test_set.append(traing_set[randIndex])</div><div class="line">        del(traing_set[randIndex])</div><div class="line"></div><div class="line">    train_martix = []</div><div class="line">    train_class = []</div><div class="line"></div><div class="line">    for doc_index in test_set:</div><div class="line">        train_martix.append(data_reader.bag_of_words_to_vec(vocal_list, doc_list[doc_index]))</div><div class="line">        train_class.append(class_list[doc_index])</div><div class="line"></div><div class="line">    pv0, pv1, pSpam = bayes.train_bayes(np.array(train_martix), np.array(train_class))</div><div class="line">    error_count = 0</div><div class="line"></div><div class="line">    for doc_index in test_set:</div><div class="line">        word_vector = data_reader.bag_of_words_to_vec(vocal_list, doc_list[doc_index])</div><div class="line">        if bayes.classify_naive_bayes(np.array(word_vector), pv0, pv1, pSpam) != class_list[doc_index]:</div><div class="line">            error_count += 1</div><div class="line">    print (&apos;the error rate is : %f&apos; %  (float(error_count) / len(test_set)))</div><div class="line"></div><div class="line">    return pv0, pv1, vocal_list</div></pre></td></tr></table></figure>
<p>​    local_words() 使用两个RSS源作为参数。然后调用函数calc_most_freq()来获得排序最高的100个单词随后将他们移除。通过下面代码进行测试，为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值。这里的错误率要远高于垃圾邮件中的错误率。由于这里关注的是单词概率而不是实际分类，因此这个问题倒不严重。可以通过函数calc_most_freq改变要移除的单词数目，然后观察错误率的变化情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">international = feedparser.parse(&apos;http://ftr.fivefilters.org/makefulltextfeed.php?url=http%3A%2F%2Fwww.feed43.com%2F1118761558281746.xml&amp;max=3&apos;)</div><div class="line">  </div><div class="line"> politics = feedparser.parse(&apos;http://news.qq.com/newsgn/rss_newsgn.xml&apos;)</div><div class="line"> </div><div class="line"> rp.get_top_words(international, politics)</div></pre></td></tr></table></figure>
<h5 id="2-分析数据-最具表征性的词汇显示函数"><a href="#2-分析数据-最具表征性的词汇显示函数" class="headerlink" title="2 分析数据:最具表征性的词汇显示函数"></a>2 分析数据:最具表征性的词汇显示函数</h5><p>​    可以先对向量international与politics进行排序，然后按照顺序将词打印出来。下面的最后一段代码会完成这部分工作。然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储。与之前返回排名最高的X个单词不同，这里可以返回大于某个阈值的所有词。这些元组会按照它们的条件概率进行排序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">def get_top_words(international, politics):</div><div class="line">    p0_v, p1_v, vocab_list = local_words(international, politics)</div><div class="line">    top_inter = []</div><div class="line">    top_politics = []</div><div class="line">    for index in range(len(p0_v)):</div><div class="line">        if float(p0_v[index]) &gt; -2.0:</div><div class="line">            top_inter.append((vocab_list[index], p0_v[index]))</div><div class="line">        if float(p1_v[index]) &gt; -2.0:</div><div class="line">            top_politics.append((vocab_list[index], p1_v[index]))</div><div class="line"></div><div class="line">    sorted_inter = sorted(top_inter, key=lambda pair: pair[1], reverse=True)</div><div class="line">    sorted_politics = sorted(top_politics, key=lambda pair: pair[1], reverse = True)</div><div class="line"></div><div class="line">    for item in sorted_inter:</div><div class="line">        print(item[0])</div><div class="line"></div><div class="line">    for item in sorted_politics:</div><div class="line">        print(item[0])</div></pre></td></tr></table></figure>
<h4 id="五-小结"><a href="#五-小结" class="headerlink" title="五 小结"></a>五 小结</h4><p>​    对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。可以通过特征之间的条件独立性假设，降低对数据量的需求。独立性假设是指一个词的出现概率并不依赖于文档中的其他词。当然我们也知道这个假设过于简单。这就是之所以称为朴素贝叶斯的原因。尽管条件独立性假设并不正确，但是朴素贝叶斯仍然是一种有效的分类器。利用现代编程语言来实现朴素贝叶斯时需要考虑很多实际因素。下溢出就是其中一个问题，它可以通过对概率取对数来解决。词袋模型在解决文档分类问题上比词集模型有所提高。还有其他一些方面的改进，比如说移除停用词，当然也可以花大量时间对切分器进行优化。本章学习到的概率理论将在后续章节中用到，另外本章也给出了有关贝叶斯概率理论全面具体的介绍。接下来的一章将暂时不再讨论概率理论这一话题，介绍另一种称作^扭此回归的分类方法及一些优化算法。        </p>
<p>​        </p>
<p>​<br>​    </p>
<p>​<br>​<br>​<br>​<br>​            </p>
<p>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​        </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​    </p>
<p>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​            </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​                </p>
<p>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2017/10/08/fifth-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
    <article id="post-fourth-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/24/fourth-article/">机器学习算法之(二)-k-means聚类算法</a>
    </h1>
  

        
        <a href="/2017/07/24/fourth-article/" class="archive-article-date">
  	<time datetime="2017-07-24T13:21:49.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-07-24</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h5 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h5><p>​    聚类是一种无监督的学习，它将相似的对象归到同一个簇中。它有点像全自动分类。聚类 方法几乎可以应用于所有对象，簇内的对象越相似，聚类的效果越好。下面介绍一种称为K-均值（k-means）聚类的算法。之所以称之为k-均值是因为它可以发现个k不同的簇，且每个簇的中心采用簇中所含值的均值计算而成。</p>
<h5 id="1-2-簇识别："><a href="#1-2-簇识别：" class="headerlink" title="1.2 簇识别："></a>1.2 簇识别：</h5><p>​    簇识别给出聚类结果的含义。假定有一些数据，现在将一些相似数据归到一起，簇识别会告诉我们这些簇到底都是些什么。聚类与分类的最大不同在于，分类的目标事先巳知，而聚类则不一样。因为其产生的结果与分类相同，而只是类别没有预先定义，聚类有时也被称为无监督分类(unsupervised classification )。聚类分析试图将相似对象归人同一簇，将不相似对象归到不同簇。相似这一概念取决于所选 择的相似度计算方法。</p>
<h3 id="2-K-均值聚类算法"><a href="#2-K-均值聚类算法" class="headerlink" title="2. K-均值聚类算法"></a>2. K-均值聚类算法</h3><blockquote>
<p>优点:容易实现。</p>
<p>缺点:可能收敛到局部最小值，在大规模数据集上收敛较慢。</p>
<p>适用数据类型:数值型数据。</p>
</blockquote>
<h5 id="2-1-K-均值聚类的流程"><a href="#2-1-K-均值聚类的流程" class="headerlink" title="2.1 K-均值聚类的流程"></a>2.1 K-均值聚类的流程</h5><p>​    K均值是发现给定数据集的K个簇的算法。簇个数K是用户给定的，每一个簇通过其质心 ( centroid) , 即簇中所有点的中心来描述。</p>
<p>​    K-均值算法的工作流程是这样的。首先，随机确定k个初始点作为质心。然后将数据集中的每个点分配到一个簇中，具体来讲，为每个点找距其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。上 面 提 到“最近”质心的说法，意味着需要进行某种距离计算。读者可以使用所喜欢的任意距离度量方法。数据集上K-均值算法的性能会受到所选距离计算方法的影响。下面给出K-均值算法的代码实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">import numpy</div><div class="line"></div><div class="line">def distEclud(vecA, vecB):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    计算两个向量的欧氏距离</div><div class="line">    :param vecA: </div><div class="line">    :param vecB: </div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    return np.sqrt(np.sum(np.power(vecA - vecB, 2)))</div><div class="line"></div><div class="line">def randCent(dataSet, k):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    给定数据集构建一个包含k个随机质心的集合。</div><div class="line">    随机质心必须要在整个数据集的边界之内，</div><div class="line">    这可以通过找到数据集每一维的最小和最大值来完成。</div><div class="line">    然 后生成0到1.0之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内</div><div class="line">    :param dataSet:  数据集</div><div class="line">    :param k:   初始质心数量</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    shape = np.shape(dataSet)</div><div class="line">    n = np.shape(dataSet)[1]</div><div class="line">    centroids = np.mat(np.zeros((k,n)))</div><div class="line"></div><div class="line">    for j in range (n) :</div><div class="line">        minJ = min(dataSet[:,j])</div><div class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)</div><div class="line">        centroids[:,j] = minJ + rangeJ * np.random.rand(k, 1)</div><div class="line"></div><div class="line">    return centroids</div></pre></td></tr></table></figure>
<p>迭代计算质心：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):</div><div class="line">    mean = np.shape(dataSet)[0]</div><div class="line">    clusterAssment = np.mat(np.zeros((mean,2)))</div><div class="line">    centroids = createCent(dataSet, k)</div><div class="line">    clusterChanged = True</div><div class="line">    while clusterChanged:</div><div class="line">        clusterChanged = False</div><div class="line">        for i in range(mean):</div><div class="line">            minDist = np.inf</div><div class="line">            minIndex = -1</div><div class="line">            for j in range(k):</div><div class="line">                distJI = distMeas(centroids[j], dataSet[i])</div><div class="line">                if distJI &lt; minDist:</div><div class="line">                    minDist = distJI</div><div class="line">                    minIndex = j</div><div class="line"></div><div class="line">            if clusterAssment[i, 0] != minIndex:</div><div class="line">                clusterChanged = True</div><div class="line"></div><div class="line">            clusterAssment[i, :] = minIndex, minDist**2</div><div class="line"></div><div class="line">        # print(centroids)</div><div class="line"></div><div class="line">        for cent in range(k):</div><div class="line">            ptsinClust = dataSet[np.nonzero(clusterAssment[:,0].A == cent)[0]]</div><div class="line">            centroids[cent,:] = np.mean(ptsinClust, axis = 0)</div><div class="line"></div><div class="line">    return centroids, clusterAssment</div></pre></td></tr></table></figure>
<p>​    kMeans()函数接受四个参数，只有数据集及簇的数目是必选参数，而用来计算距离和创建初始质心的函数都是可选的。kMeans()一开始确定数据集中数据点的总数，然后创建一个矩阵来存储每个点的簇分配结果。簇分配结果矩阵clusterAssment包含两列:一列记录簇索引值，第二列存储误差。这里的误差是指当前点到簇质心的距离，后边会使用该误差来评价聚类的效果。按照上述方式(即计算质心-分配-重新计算)反复迭代，直到所有数据点的簇分配结果不再改变为止 。</p>
<p>​    最后遍历所有质心并更新他们的取值，首先通过数组过滤来获得给定簇的所有点，遍历所有点计算均值，axis=0表示沿数组的列方向计算均值，最后返回所有类质心与点分配结果。<br>​        </p>
<h5 id="2-3-测试k-means算法："><a href="#2-3-测试k-means算法：" class="headerlink" title="2.3 测试k-means算法："></a>2.3 测试k-means算法：</h5><p>​    读取数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">def loadDataSet(self, fileName, attrType=None):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    装载数据集</div><div class="line">    :param fileName:    文件地址</div><div class="line">    :param attrType:    打开类型</div><div class="line">    :return: 数据集， 标签集</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    # 打开文件，定义编码</div><div class="line">    dataFile = open(fileName, &apos;r&apos;, encoding=&apos;utf-8&apos;)</div><div class="line">    lines = dataFile.readlines()</div><div class="line"></div><div class="line">    # 循环推导式，解析每一行，生成行数组</div><div class="line">    matrix = [self.parseLine(line, attrType=attrType) for line in lines]</div><div class="line">    # 关闭文件</div><div class="line">    dataFile.close()</div><div class="line"></div><div class="line">    # labels = matrix[0]</div><div class="line">    # dataSets = matrix[1:]</div><div class="line"></div><div class="line">    return matrix</div><div class="line"></div><div class="line">def parseLine(self, line, attrType=None):</div><div class="line">    &quot;&quot;&quot;解析行，生成列数组&quot;&quot;&quot;</div><div class="line">    # 根据制表符分割数据</div><div class="line">    # 字符串函数</div><div class="line">    words = line.strip().split(&apos;\t&apos;)</div><div class="line">    # 判断None要用is</div><div class="line">    if attrType is None:</div><div class="line">        return words</div><div class="line"></div><div class="line">    # 循环推导式，将字符串数组转换成对应类型数组</div><div class="line">    return [attrType(word) for word in words]</div></pre></td></tr></table></figure>
<p>​    数据：</p>
<p><code>1.658985    4.285136</code><br><code>-3.453687    3.424321</code><br><code>4.838138    -1.151539</code><br><code>-5.379713    -3.362104</code><br><code>0.972564    2.924086</code><br><code>-3.567919    1.531611</code><br><code>0.450614    -3.302219</code><br><code>-3.487105    -1.724432</code><br><code>2.668759    1.594842</code><br><code>-3.156485    3.191137</code><br><code>3.165506    -3.999838</code><br><code>-2.786837    -3.099354</code><br><code>4.208187    2.984927</code><br><code>-2.123337    2.943366</code><br><code>0.704199    -0.479481</code><br><code>-0.392370    -3.963704</code><br><code>2.831667    1.574018</code><br><code>-0.790153    3.343144</code><br><code>2.943496    -3.357075</code><br><code>-3.195883    -2.283926</code><br><code>2.336445    2.875106</code><br><code>-1.786345    2.554248</code><br><code>2.190101    -1.906020</code><br><code>-3.403367    -2.778288</code><br><code>1.778124    3.880832</code><br><code>-1.688346    2.230267</code><br><code>2.592976    -2.054368</code><br><code>-4.007257    -3.207066</code><br><code>2.257734    3.387564</code><br><code>-2.679011    0.785119</code><br><code>0.939512    -4.023563</code><br><code>-3.674424    -2.261084</code><br><code>2.046259    2.735279</code><br><code>-3.189470    1.780269</code><br><code>4.372646    -0.822248</code><br><code>-2.579316    -3.497576</code><br><code>1.889034    5.190400</code><br><code>-0.798747    2.185588</code><br><code>2.836520    -2.658556</code><br><code>-3.837877    -3.253815</code><br><code>2.096701    3.886007</code><br><code>-2.709034    2.923887</code><br><code>3.367037    -3.184789</code><br><code>-2.121479    -4.232586</code><br><code>2.329546    3.179764</code><br><code>-3.284816    3.273099</code><br><code>3.091414    -3.815232</code><br><code>-3.762093    -2.432191</code><br><code>3.542056    2.778832</code><br><code>-1.736822    4.241041</code><br><code>2.127073    -2.983680</code><br><code>-4.323818    -3.938116</code><br><code>3.792121    5.135768</code><br><code>-4.786473    3.358547</code><br><code>2.624081    -3.260715</code><br><code>-4.009299    -2.978115</code><br><code>2.493525    1.963710</code><br><code>-2.513661    2.642162</code><br><code>1.864375    -3.176309</code><br><code>-3.171184    -3.572452</code><br><code>2.894220    2.489128</code><br><code>-2.562539    2.884438</code><br><code>3.491078    -3.947487</code><br><code>-2.565729    -2.012114</code><br><code>3.332948    3.983102</code><br><code>-1.616805    3.573188</code><br><code>2.280615    -2.559444</code><br><code>-2.651229    -3.103198</code><br><code>2.321395    3.154987</code><br><code>-1.685703    2.939697</code><br><code>3.031012    -3.620252</code><br><code>-4.599622    -2.185829</code><br><code>4.196223    1.126677</code><br><code>-2.133863    3.093686</code><br><code>4.668892    -2.562705</code><br><code>-2.793241    -2.149706</code><br><code>2.884105    3.043438</code><br><code>-2.967647    2.848696</code><br><code>4.479332    -1.764772</code><br><code>-4.905566    -2.911070</code></p>
<p>​    对datMat中的数据点进行聚类处理，从图像中可以大概预先知道最后的结果应该有4个簇。运行程序，经过4次迭代，算法收敛。</p>
<h5 id="2-4-使用后处理来提高聚类性能"><a href="#2-4-使用后处理来提高聚类性能" class="headerlink" title="2.4 使用后处理来提高聚类性能"></a>2.4 使用后处理来提高聚类性能</h5><p>​    前面提到，在K均值聚类中簇的数目k是一个用户预先定义的参数，那么用户如何才能知道k的选择是否正确?如何才能知道生成的簇比较好呢?在包含簇分配结果的矩阵中保着每个点的误差，即该点到簇质心的距离平方值。下面会讨论利用该误差来评价聚类质量的方法。</p>
<p>​    一般用于度量聚类效果的指标是SSE（误差平方和），SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持族数目不变的情况下提高簇的质量。</p>
<p>​    K-均值算法收敛但聚类效果较差的原因是，K-均值算法收敛到了局部最小值，而非全局最小值(局部最小值指结果还可以但并非最好结果，全局最小值是可能的最好结果)。                    </p>
<p>​    那么如何对以上程序进行改进，可以对生成的簇进行后处理，一种方法是可以将具有最大SSE的簇划分成两个。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法。</p>
<h5 id="2-5-二分K均值算法"><a href="#2-5-二分K均值算法" class="headerlink" title="2.5 二分K均值算法"></a>2.5 二分K均值算法</h5><p>​    为克服K-均值算法收敛域局部最小值，有人提出了二分K均值算法。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划，一种方法是对”其划分是否可以最大程度降低SSE的值”。上述基于SSE的划分过程不断重复，直到得到用户指定的簇数目为止。另一种做法是选择SSE最大的簇进行划分，直到簇数目达到用户指定的数目为止。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">def biKMeans(dataSet, k, distMeas=utils.distEclud):</div><div class="line">    num = np.shape(dataSet)[0]</div><div class="line">    clusterAssment = np.mat(np.zeros((num, 2)))</div><div class="line">    centroid_first = np.mean(dataSet, axis=0).tolist()[0]</div><div class="line">    centList = [centroid_first]</div><div class="line"></div><div class="line">    for j in range(num):</div><div class="line">        clusterAssment[j, 1] = distMeas(np.mat(centroid_first), dataSet[j, :]) ** 2</div><div class="line"></div><div class="line">    while(len(centList) &lt; k):</div><div class="line">        lowestSSE = np.inf</div><div class="line">        bestCentToSplit = 0</div><div class="line">        for i in range(len(centList)):</div><div class="line">            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, 0].A == i)[0], :]</div><div class="line">            centroidMat, splitClustAss = kMeans.kMeans(dataSet, 2, distMeas)</div><div class="line">            sseSplit = np.sum(splitClustAss[:, 1])</div><div class="line">            sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, 0].A != i)[0], 1])</div><div class="line"></div><div class="line">            if (sseSplit + sseNotSplit) &lt; lowestSSE:</div><div class="line">                bestCentToSplit = i</div><div class="line">                bestNewCents = centroidMat</div><div class="line">                bestClustAss = splitClustAss.copy()</div><div class="line">                lowestSSE = sseSplit + sseNotSplit</div><div class="line"></div><div class="line">        matrix = bestClustAss[:, 0].A</div><div class="line">        index_list = np.nonzero(bestClustAss[:, 0].A == 1)[0]</div><div class="line"></div><div class="line">        bestClustAss[np.nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)</div><div class="line">        bestClustAss[np.nonzero(bestClustAss[:, 0].A == 0)[0], 0] = len(centList)</div><div class="line"></div><div class="line">        print(&apos;the bestCentToSplit is %d&apos; % bestCentToSplit)</div><div class="line"></div><div class="line">        centList[bestCentToSplit] = bestNewCents[0, :]</div><div class="line">        centList.append(bestNewCents[1, :])</div><div class="line">        clusterAssment[np.nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss</div><div class="line"></div><div class="line">    return np.mat(centList), clusterAssment</div></pre></td></tr></table></figure>
<p>​    在给定数据集、所期望的簇数目和距离计算方法的条件下，函数返回聚类结果。同咖kMeans一样，用户可以改变所使用的距离计算方法。该函数首先创建一个矩阵来存储数据集中每个点的簇分配结果及平方误差，然后计算整个数据集的质心，并使用一个列表来保留所有的质心。得到初始质心后，可以遍历数据集中所有的点来计算每个点到质心的误差。</p>
<p>​    接下来进入循环对簇进行划分，知道得到想要的簇数目为止。可以通过考察簇列表中的值来获得当前簇的数目，然后遍历所有的簇来决定最佳的簇进行划分。为此需要比较划分前后的SSE，一开始将最小SSE设置为无穷大，然后遍历簇列表centList中的每一个簇。对每个簇中的所有点看成一个小的数据集ptsInCurrCluster，输入到函数kMeans中进行处理（k = 2）。k均值算法将会生成两个质心，同时计算每个簇的误差值。这些误差与剩余数据集的误差之和作为本次划分的误差。如果该划分的SSE值最小，则本次划分被保存。一旦决定了要划分的簇，接下来就要实际执行划分操作。划分操作很容易，只需要将要划分的簇中所有点的簇分配结果进行修改即可。当使用<br>kMeans ()函数并且指定簇数为2时 ，会得到两个编号分别为0和1的结果簇。需要将这些簇编号修改为划分簇及新加簇的编号，该过程可以通过两个数组过滤器来完成。最后新的簇分配结果被更新，新的质心会被添加到centList中。                        </p>
<h3 id="3-对地图上的点进行聚类"><a href="#3-对地图上的点进行聚类" class="headerlink" title="3. 对地图上的点进行聚类"></a>3. 对地图上的点进行聚类</h3><p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​<br>​    </p>
<p>​<br>​    </p>
<p>​    </p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2017/07/24/fourth-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
    <article id="post-third-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/24/third-article/">机器学习算法之(一)--决策树</a>
    </h1>
  

        
        <a href="/2017/07/24/third-article/" class="archive-article-date">
  	<time datetime="2017-07-24T04:46:28.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-07-24</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="一-概述"><a href="#一-概述" class="headerlink" title="一 . 概述"></a>一 . 概述</h3><p>决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。</p>
<p>决策树很多任务都 是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则 ，机器学习算法最终将使用这些机器从数据集中创造的规则。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。</p>
<blockquote>
<p>优点:计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特 征数据。</p>
<p>缺点 :可能会产生过度匹配问题。 </p>
<p>适用数据类型:数值型和标称型</p>
</blockquote>
<h3 id="二-决策树的构造"><a href="#二-决策树的构造" class="headerlink" title="二. 决策树的构造"></a>二. 决策树的构造</h3><h5 id="2-1-构造流程-决策树的一般流程"><a href="#2-1-构造流程-决策树的一般流程" class="headerlink" title="2.1. 构造流程: 决策树的一般流程"></a>2.1. 构造流程: 决策树的一般流程</h5><ol>
<li>收集数据:可以使用任何方法。 </li>
<li>准备数据:树构造算法只适用于标称型数据，因此数值型数据必须离散化。 </li>
<li>分析数据:可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 </li>
<li>训练算法:构造树的数据结构。</li>
<li>测试算法:使用经验树计算错误率。</li>
<li>使用算法:此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。</li>
</ol>
<p>​        在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类， 无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内。</p>
<h5 id="2-2-信息增益"><a href="#2-2-信息增益" class="headerlink" title="2.2.  信息增益"></a>2.2.  信息增益</h5><p>​    划分数据集的大原则是:将无序的数据变得更加有序。我们可以使用多种方法划分数据集， 但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息 论是量化处理信息的分支科学。我们可以在划分数据之前使用信息论量化度量信息的内容。</p>
<p>​    在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以 计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。在可以评测哪种数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集 合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德•香农。</p>
<p>​    熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事 务可能划分在多个分类之中’ 则符号^的信息定义为</p>
<p>$$<br>l(Xi) = -log2p(Xi)<br>$$</p>
<p>其中p(Xi) 是 选择了该分类的概率，为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到:</p>
<p><img src="/2017/07/24/third-article/CodeCogsEqn3_1.png" alt=""></p>
<p>python实现计算给定数据集的熵:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">from math import log</div><div class="line"></div><div class="line">def calcShannonEnt(dataSet):</div><div class="line"></div><div class="line">    numEntries = len(dataSet)</div><div class="line">    labelCounts = &#123;&#125;</div><div class="line"></div><div class="line">    for featVec in dataSet:</div><div class="line">        currentLabel = featVec[-1]</div><div class="line">        if currentLabel not in labelCounts.keys():</div><div class="line">            labelCounts[currentLabel] = 0</div><div class="line">            labelCounts[currentLabel] += 1</div><div class="line"></div><div class="line">    shannonEnt = 0.0</div><div class="line">    for key in labelCounts:</div><div class="line">        prob = float (labelCounts [key] ) / numEntries</div><div class="line">        shannonEnt -= prob * log(prob,2)</div><div class="line">        </div><div class="line">    return shannonEnt</div></pre></td></tr></table></figure>
<p>​    首先,计算数据集中实例的总数。我们也可以在需要时再计算这个值，但是由于代码中多次用到这个值，为了提高代码效率，我们显式地声明一个变量保存实例总数。然后，创建一个数据字典，它的键值是最后一列的数值0 。如果当前键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。最 后 ，使用所有类标签的发生频率计算类别出现的概率。我们将用这个概率计算香农熵，统计所有类标签发生的次数。下面我们看看如何使用熵划分数据集。</p>
<p>​    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def createDataSet():</div><div class="line">    dataSet = [[1, 1, &apos;yes&apos;],</div><div class="line">               [1, 1, &apos;yes&apos;],</div><div class="line">               [1, 0, &apos;no&apos;],</div><div class="line">               [0, 1, &apos;no&apos;],</div><div class="line">               [0, 1, &apos;no&apos;]]</div><div class="line">    labels = [&apos;no surfacing&apos;, &apos;flippers1&apos;]</div><div class="line"></div><div class="line">    return dataSet, labels</div></pre></td></tr></table></figure>
<p>​    </p>
<h5 id="2-3-划分数据集："><a href="#2-3-划分数据集：" class="headerlink" title="2.3 划分数据集："></a>2.3 划分数据集：</h5><p>​    上节我们学习了如何度量数据集的无序程度，分类算法除了需要测量信息熵，还需要划分数<br>据集，度量花费数据集的熵，以便判断当前是否正确地划分了数据集。我们将对每个特征划分数<br>据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。想象一个分<br>布在二维空间的数据散点图，需要在数据之间划条线，将它们分成两部分，我们应该按照X轴还<br>是Y轴划线呢.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">def splitDataSet(dataSet, axis, value):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    :param dataSet: 带划分的数据集</div><div class="line">    :param axis:    划分数据集的特征</div><div class="line">    :param value:   特征的返回值</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    retDataSet = []</div><div class="line">    for featVec in dataSet:</div><div class="line">        if featVec[axis] == value:</div><div class="line">            reducedFeatVec = featVec[:axis]</div><div class="line">            reducedFeatVec.extend(featVec[axis+1:])</div><div class="line">            retDataSet.append(reducedFeatVec)</div><div class="line"></div><div class="line">    return retDataSet</div></pre></td></tr></table></figure>
<p>​    以上程序    需要注意的是，python语言不用考虑内存分配问题。python在函数中传递的是列表的引用，在函数内部对列表对象的修改，将会影响该列表对象的整个生存周期。为了消除这个不良影 响 ，我们需要在函数的开始声明一个新列表对象。因为该函数代码在同一数据集上被调用多次，为了不修改原始数据集，创建一个新的列表对象0 。数据集这个列表中的各个元素也是列表，我们要遍历数据集中的每个元素，一旦发现符合要求的值，则将其添力到新创建的列表中。代码中使用了Python extend() 和 append() 方法。这两个方法功能类似，但是在处理多个列表时，这两个方法的处理结果是完全不同的。</p>
<p>​    假定存在两个列表，a:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = [1,2,3] </div><div class="line">&gt;&gt;&gt; b = [4,5,6]</div><div class="line">&gt;&gt;&gt; a.append(b)</div><div class="line">&gt;&gt;&gt; a</div><div class="line">[1, 2, 3, [4, 5, 6]]</div></pre></td></tr></table></figure>
<p>如果执行a.append(b)，则列表得到了第四个元素，而且第四个元素也是一个列表。然而如果使用extend方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a= [1,2,3]</div><div class="line">&gt;&gt;&gt; a .extend(b)</div><div class="line">[1, 2, 3, 4, 5, 6]</div></pre></td></tr></table></figure>
<p>则得到一个包含a和b所有元素的列表.</p>
<p>​    接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。熵计算将会告诉我们如何划分数据集是最好的数据组织方式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def chooseBestFeatureToSplit(dataSet):</div><div class="line">    numFeatures = len(dataSet[0]) - 1</div><div class="line">    baseEntropy = calcShannonEnt(dataSet)</div><div class="line">    bestInfoGain = 0.0</div><div class="line">    bestFeature = -1</div><div class="line"></div><div class="line">    for i in range(numFeatures):</div><div class="line">        featList = [example[i] for example in dataSet]</div><div class="line">        uniqueValues = set(featList)</div><div class="line">        newEntropy = 0.0</div><div class="line">        for value in uniqueValues:</div><div class="line">            subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">            prob = len(subDataSet) / float(len(dataSet))</div><div class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</div><div class="line"></div><div class="line">        infoGain = baseEntropy - newEntropy</div><div class="line"></div><div class="line">        if (infoGain &gt; bestInfoGain):</div><div class="line">            bestInfoGain = infoGain</div><div class="line">            bestFeature = i</div><div class="line"></div><div class="line">    return bestFeature</div></pre></td></tr></table></figure>
<p>​    chooseBestFeatureToSplit() 实现了选取特征，划分数据集，计算最好的划分数据及特征。在函数中调用的数据需要满足一定的要求:第一个要求是，数据必须是一种由列表元素组成的列表，而且所有的列表元素都要具有相同的数据长度;第二个要求是，数据的最后一列或者每个实例的最后一个元素是当前实例的类别标签。数据集一<br>旦满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需限定list 中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算.</p>
<p>​    开始划分数据集之前，计算了整个数据集的原始香农熵，我们保存最初的无序度量值,.用于与划分完之后的数据集计算的熵值进行比较。第1个for 循环遍历数据集中的所有特征, 使用列表推导创建新的列表。建立集合数据。遍历当前特征中的所以唯一属性，对每个特征划分一次数据集。然后计算数据集的新熵值 ，并对所有唯一特征值得到的熵求和。最后比较所有特征中的信息增益，返回最好特征划分的索引值。</p>
<h5 id="2-4-递归构建决策树"><a href="#2-4-递归构建决策树" class="headerlink" title="2.4 递归构建决策树"></a>2.4 递归构建决策树</h5><p>​    目前我们已经学习了从数据集构造决策树算法，如果数据集已经处理了所有属性，但是类标签依然不是唯一<br>的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def majorityCnt(classList):</div><div class="line">    classCount=&#123;&#125;</div><div class="line">    for vote in classList:</div><div class="line">        if vote not in classCount.keys():</div><div class="line">            classCount[vote] = 0</div><div class="line">        classCount[vote] += 1</div><div class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)</div><div class="line"></div><div class="line">    return sortedClassCount[0][0]</div></pre></td></tr></table></figure>
<p>​<br>​    该函数使用分类名称的列表，然后创建键值为classList中唯一值的数据字典，字典对象存储了classList中每个类标签出现的频率，最后利用operator操作键值排序字典，并返回出现次数最多的分类名称。</p>
<p>​    创建树：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">def createTree(dataSet, labels):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    :param dataSet:  数据集 </div><div class="line">    :param labels:  标签列表 （包含了数据集中所有 特征的标签，算法本身并不需要这个变量，</div><div class="line">                                但是为了给出数据明确的含义，我们将它作为一个输 人参数提供）</div><div class="line">    :return: </div><div class="line">    &apos;&apos;&apos;</div><div class="line">    classList = [example [-1] for example in dataSet]</div><div class="line">    if classList.count (classList [0] ) == len(classList) :</div><div class="line">        return classList[0]</div><div class="line"></div><div class="line">    if len(dataSet[0]) == 1:</div><div class="line">        return majorityCnt(classList)</div><div class="line"></div><div class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">    bestFeatLabel = labels[bestFeat]</div><div class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</div><div class="line"></div><div class="line">    del(labels[bestFeat])</div><div class="line">    featValues = [example[bestFeat] for example in dataSet]</div><div class="line">    uniqueVals = set(featValues)</div><div class="line"></div><div class="line">    for value in uniqueVals:</div><div class="line">        subLabels = labels[:]</div><div class="line">        myTree[bestFeatLabel] [value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</div><div class="line"></div><div class="line">    return myTree</div></pre></td></tr></table></figure>
<p>​    递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签0 。递归函数的第二个停止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。由于第二个条件无法简单地返回唯一的类标签 ，这里使用majorityCnt()函数挑选出现次数最多的类别作为返回值。</p>
<p>​    下一步程序开始创建树，这里使用?字典类型存储树的信息，当然也可以声明特殊的数据类型存储树，但是这里完全没有必要。字典变量myTree存储了树的所有信息，这对于其后绘制树形图非常重要。当前数据集选取的最好特征存储在变量bestFeat 中，得到列表包含的所有属性值.</p>
<p>​    最后代码遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree,    , 得 到 的 返 回 值 将 被 插 人 到 字 典 变 量 myTree中 ， 因 此 函 数 终 止 执 行 时 ，字典中将会嵌套很多代表叶子节点信息的字典数据.</p>
<h3 id="三-测试和存储分类器"><a href="#三-测试和存储分类器" class="headerlink" title="三. 测试和存储分类器"></a>三. 测试和存储分类器</h3><h5 id="3-1-测试算法：使用决策树执行分类"><a href="#3-1-测试算法：使用决策树执行分类" class="headerlink" title="3.1 测试算法：使用决策树执行分类"></a>3.1 测试算法：使用决策树执行分类</h5><p>​    依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然 后，程序比较测试数据与决策树上的数值，递归执行该过程直到进人叶子节点;最后将测试数据定义为叶子节点所属的类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def classify(inputTree, featLabels, testVec):</div><div class="line">    firstStr = inputTree.keys()[0]</div><div class="line">    secondDict = inputTree[firstStr]</div><div class="line">    featIndex = featLabels.index(firstStr)</div><div class="line"></div><div class="line">    for key in secondDict.keys():</div><div class="line">        if testVec[featIndex] == key:</div><div class="line">            if type(secondDict[key]).__name__ == &apos;dict&apos;:</div><div class="line">                classLabel = classify(secondDict[key], featLabels, testVec)</div><div class="line">            else:</div><div class="line">                classLabel = secondDict[key]</div><div class="line"></div><div class="line">    return classLabel</div></pre></td></tr></table></figure>
<p>​    在存储带有特征的数据会面临一个问题:程序无法确定特征在数据集中的位置，例如前面例子的第一个用于划分数据集的特征是nosurfacing属性 ，但是在实际数据集中该属性存储在哪个位置?是第一个属性还是第二个属性? 特征标签列表将帮助程序处理这个问题。使用index方法查找当前列表中第一个匹配firstStr变量的元素。然后代码递归遍历整棵树，比较testVec变量中的值与树节点的值，如果到达叶子节点，则返回当前节点的分类标签。</p>
<h5 id="3-2-使用算法：决策树的存储"><a href="#3-2-使用算法：决策树的存储" class="headerlink" title="3.2 使用算法：决策树的存储"></a>3.2 使用算法：决策树的存储</h5><p>​    构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的<br>时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，贝何以<br>很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用巳经构造好的决策树。为<br>了解决这个问题，需要使用python模块pickle 序列化对象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def storeTree(inputTree, filename):</div><div class="line">    fw = open(filename, &apos;wb&apos;)</div><div class="line">    pickle.dump(inputTree, fw)</div><div class="line">    fw.close()</div><div class="line"></div><div class="line">def grabTree(filename):</div><div class="line">    fr = open(filename, &apos;rb&apos;)</div><div class="line">    return pickle.load(fr)</div></pre></td></tr></table></figure>
<h3 id="四-使用决策树预测是否打球"><a href="#四-使用决策树预测是否打球" class="headerlink" title="四. 使用决策树预测是否打球"></a>四. 使用决策树预测是否打球</h3><p>​    使用数据集如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Weather	Temperature	Humidity	Windy	Class</div><div class="line">Rain	Hot	High	Weak	No</div><div class="line">Rain	Hot	High	Strong	No</div><div class="line">Overcast	Hot	High	Weak	Yes</div><div class="line">Sunny	Mild	High	Weak	Yes</div><div class="line">Sunny	Cool	Normal	Weak	Yes</div><div class="line">Sunny	Cool	Normal	Strong	No</div><div class="line">Overcast	Cool	Normal	Strong	Yes</div><div class="line">Rain	Mild	High	Weak	No</div><div class="line">Rain	Cool	Normal	Weak	Yes</div><div class="line">Sunny	Mild	Normal	Weak	Yes</div><div class="line">Rain	Mild	Normal	Strong	Yes</div><div class="line">Overcast	Mild	High	Strong	Yes</div><div class="line">Overcast	Hot	Normal	Weak	Yes</div><div class="line">Sunny	Mild	High	Strong	No</div></pre></td></tr></table></figure>
<p>​    读取数据集代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">def loadDataSet(self, fileName, attrType=None):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    装载数据集</div><div class="line">    :param fileName:    文件地址</div><div class="line">    :param attrType:    打开类型</div><div class="line">    :return: 数据集， 标签集</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    # 打开文件，定义编码</div><div class="line">    dataFile = open(fileName, &apos;r&apos;, encoding=&apos;utf-8&apos;)</div><div class="line">    lines = dataFile.readlines()</div><div class="line"></div><div class="line">    # 循环推导式，解析每一行，生成行数组</div><div class="line">    matrix = [self.parseLine(line, attrType=attrType) for line in lines]</div><div class="line">    # 关闭文件</div><div class="line">    dataFile.close()</div><div class="line"></div><div class="line">    labels = matrix[0]</div><div class="line">    dataSets = matrix[1:]</div><div class="line"></div><div class="line">    return dataSets, labels</div><div class="line"></div><div class="line">def parseLine(self, line, attrType=None):</div><div class="line">    &quot;&quot;&quot;解析行，生成列数组&quot;&quot;&quot;</div><div class="line">    # 根据制表符分割数据</div><div class="line">    # 字符串函数</div><div class="line">    words = line.strip().split(&apos;\t&apos;)</div><div class="line">    # 判断None要用is</div><div class="line">    if attrType is None:</div><div class="line">        return words</div><div class="line"></div><div class="line">    # 循环推导式，将字符串数组转换成对应类型数组</div><div class="line">    return [attrType(word) for word in words]</div></pre></td></tr></table></figure>
<h3 id="五-总结："><a href="#五-总结：" class="headerlink" title="五. 总结："></a>五. 总结：</h3><p>​    决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首<br>先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的<br>所有数据属于同一分类。见ID3算法可以用于划分标称型数据集。构建决策树时，我们通常采用递<br>归的方法将数据集转化为决策树。</p>
<p>​<br>​<br>​    </p>
<p>​    </p>
<p>​<br>​<br>​<br>​<br>​                </p>
<p>​<br>​<br>​<br>​    </p>
<p>​    </p>
<p>​            </p>
<p>​    </p>
<p>​<br>​<br>​    </p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2017/07/24/third-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
    <article id="post-second-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/06/20/second-article/">机器学习历程</a>
    </h1>
  

        
        <a href="/2017/06/20/second-article/" class="archive-article-date">
  	<time datetime="2017-06-19T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-06-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-数学"><a href="#1-数学" class="headerlink" title="1 数学"></a>1 数学</h2><ul>
<li><p>线性代数：（10天）</p>
<p>  线性代数大学教材和视频，深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。</p>
</li>
</ul>
<blockquote>
<p>资料：</p>
<p>同济大学线性代数 ：（现代基础）(行列式和矩阵计算为主)（5天）</p>
<p>Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang. （5天）</p>
</blockquote>
<ul>
<li><p>概率论与数理统计（45天）（一小时）</p>
<p>  概率论与数理统计基础知识，掌握机器学习相关的经典数理统计方法，熟悉Linear regression(线性回归)，factor analysis（因子分析），principal component analysis（主成分分析），canonical component analysis（典型相关分析），马尔可夫概率分布，之后就可以进一步深入学习贝叶斯统计和Graphical models。</p>
</li>
</ul>
<blockquote>
<p>资料<br>浙江大学概率论与数理统计第四版加视频（基础知识）（30天）<br>Introduction to Graphical Models (draft version).  by M. Jordan and C. Bishop.(贝叶斯概率) （参考）<br>计量经济学导论(回归分析及应用) （15天）</p>
</blockquote>
<ul>
<li>其他数学相关<br>  当然研究机器学习不止需要上述数学知识，例如分析学(Analysis)，拓扑 (Topology)都有可能用到，这些知识可以用到的时候再去研究</li>
</ul>
<h2 id="2-算法"><a href="#2-算法" class="headerlink" title="2 算法"></a>2 算法</h2><ul>
<li><p>机器学习（45天）（每天一小时）</p>
<p>  有许多经典算法，当然掌握这些算法前先要熟悉计算机常用数据结构和基础算法，例如树，图的简历和遍历，之后在深入到机器学习几大重要算法决策树（Decision Trees），朴素贝叶斯分类(Naive Bayesian classification)，逻辑回归(Logistic Regression)， 支持向量机（Support Vector Machine，SVM），线性回归，最近邻算法——KNN，等等</p>
</li>
</ul>
<blockquote>
<p>资料<br><a href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/（算法比较）" target="_blank" rel="external">http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/（算法比较）</a></p>
<p>清华大学数据结构（熟悉树和图数据结构）（15天）</p>
<p>机器学习 周志华 （30天）持续学习和Python的学习穿插进行，使用python完成一些模型。</p>
</blockquote>
<ul>
<li><p>神经网络(30天)</p>
<p> 神经网络是机器学习的核心，主要有CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)</p>
</li>
</ul>
<blockquote>
<p>资料<br><a href="http://deeplearning.net/tutorial/" target="_blank" rel="external">http://deeplearning.net/tutorial/</a></p>
<p>神经⽹络与深度学习（30天）</p>
<p>Deep Convolutional Network</p>
<p><a href="http://open.163.com/special/opencourse/machinelearning.html(斯坦福公开课" target="_blank" rel="external">http://open.163.com/special/opencourse/machinelearning.html(斯坦福公开课</a> 吴恩达)</p>
</blockquote>
<h2 id="3-编程语言"><a href="#3-编程语言" class="headerlink" title="3  编程语言"></a>3  编程语言</h2><ul>
<li><p>Python（15天）</p>
<p>  对于有编程经验的同学，找本Python经典书籍快速浏览，主要在使用中进行熟悉。当然其他编程语言依然可以实现机器学习，但对于语言的学习都是大同小异吧</p>
</li>
</ul>
<blockquote>
<p>资料</p>
<p>Learning Python, 5th Edition</p>
</blockquote>
<h2 id="4-构建工具"><a href="#4-构建工具" class="headerlink" title="4 构建工具"></a>4 构建工具</h2><ul>
<li><p>tensorFlow (30天)（每天2小时）</p>
<p>  可以使用TensorFlow进行建模和机器训练</p>
<blockquote>
<p>资料<br><a href="http://www.tensorfly.cn/tfdoc/tutorials/overview.html" target="_blank" rel="external">http://www.tensorfly.cn/tfdoc/tutorials/overview.html</a> </p>
</blockquote>
</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2017/06/20/second-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
    <article id="post-first-article" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/05/21/first-article/">解决切换controller后GrowingIO统计页面问题</a>
    </h1>
  

        
        <a href="/2017/05/21/first-article/" class="archive-article-date">
  	<time datetime="2017-05-20T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-05-21</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前几日产品经理查看GrowingIO数据时发现了我们应用首页的一个问题，页面如下</p>
<p><img src="/Users/leijia/workspace/jialeigd/source/_posts/homepage.jpeg" alt="首页"></p>
<p>产品首页上部分为三个tab，分别对应精选，关注，发现三个controller，每个controller以addChildViewController的方式加入到首页controller中. 但是问题来了，最后add到parentController的ChildController会覆盖前两个controller，导致切换其他两个controller在growingIO统计到的都是最后一个发现controller.</p>
<h3 id="1-如何切换当前childController"><a href="#1-如何切换当前childController" class="headerlink" title="1. 如何切换当前childController"></a>1. 如何切换当前childController</h3><p>经查阅文档，iOS官方提供了切换subController的方法：</p>
<pre><code>- (void)transitionFromViewController:(UIViewController *)fromViewController toViewController:(UIViewController *)toViewController duration:(NSTimeInterval)duration options:(UIViewAnimationOptions)options animations:(void (^ __nullable)(void))animations completion:(void (^ __nullable)(BOOL finished))completion
</code></pre><blockquote>
<p>Transitions between two of the view controller’s child view controllers.<br>This method adds the second view controller’s view to the view hierarchy and then performs the animations defined in your animations block. After the animation completes, it removes the first view controller’s view from the view hierarchy.<br>This method is only intended to be called by an implementation of a custom container view controller. If you override this method, you must call super in your implementation.在两个子视图控制器中转换该方法，执行完以后，fromViewController指代的视图控制器的View将从界面消失；toViewController所指代的视图控制器的View将被载入到页面中</p>
</blockquote>
<p>有了此方法，就可以在scrollView滑动页面切换页面停止滚动后对当前ChilidViewController进行切换，那么问题又来了：</p>
<h3 id="2-如何检测scrollView停止了滑动？"><a href="#2-如何检测scrollView停止了滑动？" class="headerlink" title="2.如何检测scrollView停止了滑动？"></a>2.如何检测scrollView停止了滑动？</h3><p>很遗憾，iOS没有直接提供这样的API。 先看一下现有的几个方法是怎样的：</p>
<ol>
<li><p>(void)scrollViewDidEndDragging:(UIScrollView *)scrollView willDecelerate:(BOOL)decelerate;</p>
<p>这个方法表示手指离开了scrollview，第二个参数用于判断滚动速度是否慢慢下降。</p>
</li>
<li><p>(void)scrollViewDidEndDecelerating:(UIScrollView *)scrollView;      // called when scroll view grinds to a halt 这个方法看上去挺像我们要找的：停止减速。但是，从注释看，“嘎然而止”的时候才会被调用，很明显，我们要的是“自然停止”时被调的方法。</p>
</li>
<li><p>(void)scrollViewDidEndScrollingAnimation:(UIScrollView *)scrollView; // called when setContentOffset scrollRectVisible:animated: finishes. not called if not animating 这个是指scrollview停止滚动动画，嗯，这下是我们想要的了吧！抱歉，不是！！！ 试验一下就会发现，无论怎么滚动怎么停止，这个方法都不会被调！除非用代码的方式调用了setContentOffset scrollRectVisible:animated: finishes，但是，我们手指触发试图滚动是不会调该方法的……</p>
</li>
</ol>
<p>研究了一通，一无所获啊，先打局德州冷静下，就在一局结束之时，突然灵光乍现， 苹果并没有提供直接的方法判断scrollView自然停止，只能搞点奇巧淫技来解决了，想起之前用到的一个系统方法：</p>
<pre><code>+ (void)cancelPreviousPerformRequestsWithTarget:(id)aTarget selector:(SEL)aSelector object:(id)anArgument
</code></pre><blockquote>
<p>All perform requests are canceled that have the same target as aTarget, argument as anArgument, and selector as aSelector. This method removes perform requests only in the current run loop, not all run loops.停止当前runloop下之前要执行的selector。</p>
</blockquote>
<p>有了此方法，就可以在- (void)scrollViewDidScroll:(UIScrollView *)scrollView最后不断通过- (void)performSelector:(SEL)aSelector withObject:(nullable id)anArgument afterDelay:(NSTimeInterval)delay;去延迟执行一个自己编写的scrollView停止方法，如果执行到此方法说明滚动停止，但是要在performSelector之前调用cancelPreviousPerformRequestsWithTarget去停止此方法，这就达到了如果滚动没有停止就通过cancelPreviousPerformRequestsWithTarget去取消停止方法的执行，如果滚动停止了，那么scrollViewDidScroll不再执行，本次的cancelPreviousPerformRequestsWithTarget也不执行，就无法取消上次的performSelector去执行真正的停止方法，达到了监测scrollView自然停止滚动的效果，代码如下：</p>
<pre><code>- (void)scrollViewDidScroll:(UIScrollView *)scrollView
{
   //检查scrollView是否停止
    [NSObject cancelPreviousPerformRequestsWithTarget:self];
    [self performSelector:@selector(scrollViewDidEndScroll:) withObject:nil afterDelay:0.3];
}
</code></pre><p>以上就是通过监测scrollView停止滚动后来切换parentVC的childVC来判断当前所选择的viewController</p>
<hr>
<p>2018-1-18 更新</p>
<p><code>(void)transitionFromViewController:(UIViewController *)fromViewController toViewController:(UIViewController *)toViewController duration:(NSTimeInterval)duration options:(UIViewAnimationOptions)options animations:(void (^ __nullable)(void))animations completion:(void (^ __nullable)(BOOL finished))completion</code></p>
<p>通过此方法切换controller生命周期会导致subView消失的问题，必须再手动addSubview才可以让子view显示。经过调研发现了以下方法</p>
<p><code>// If a custom container controller manually forwards its appearance callbacks, then rather than calling viewWillAppear:, viewDidAppear: viewWillDisappear:, or viewDidDisappear: on the children these methods should be used instead. This will ensure that descendent child controllers appearance methods will be invoked. It also enables more complex custom transitions to be implemented since the appearance callbacks are</code></p>
<p><code>// now tied to the final matching invocation of endAppearanceTransition.</code></p>
<p><code>\- (void)beginAppearanceTransition:(BOOL)isAppearing animated:(BOOL)animated __OSX_AVAILABLE_STARTING(__MAC_NA,__IPHONE_5_0);</code></p>
<p><code>\- (void)endAppearanceTransition __OSX_AVAILABLE_STARTING(__MAC_NA,__IPHONE_5_0);</code></p>
<p>可以在(void)scrollViewDidEndScroll:(UIScrollView*)scrollView方法中，通过beginAppearanceTransition 开启controller的生命周期，endAppearanceTransition() 方法来结束生命周期。达到切换不同controller生命周期的目的。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2017/05/21/first-article/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>





  
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2018 贾磊的猜想
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(r){if(e[r])return e[r].exports;var o=e[r]={exports:{},id:r,loaded:!1};return t[r].call(o.exports,o,o.exports,n),o.loaded=!0,o.exports}var e={};return n.m=t,n.c=e,n.p="./",n(0)}([function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}function o(t,n){var e=/\/|index.html/g;return t.replace(e,"")===n.replace(e,"")}function i(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,e=0,r=t.length;e<r;e++){var i=t[e];o(n,i.getAttribute("href"))&&(0,d.default)(i,"active")}}function u(t){for(var n=t.offsetLeft,e=t.offsetParent;null!==e;)n+=e.offsetLeft,e=e.offsetParent;return n}function f(t){for(var n=t.offsetTop,e=t.offsetParent;null!==e;)n+=e.offsetTop,e=e.offsetParent;return n}function c(t,n,e,r,o){var i=u(t),c=f(t)-n;if(c-e<=o){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,h.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(e||c)+"px",a.style.left=i+"px",a.style.zIndex=r||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");c(t,document.body.scrollTop,-63,2,0),c(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}function l(){x.default.versions.mobile&&window.screen.width<800&&(i(),s())}var p=e(71),d=r(p),v=e(72),y=(r(v),e(84)),h=r(y),b=e(69),x=r(b),g=e(75),m=r(g),w=e(70);l(),(0,w.addLoadEvent)(function(){m.default.init()}),t.exports={}},function(t,n){var e=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=e)},function(t,n){var e={}.hasOwnProperty;t.exports=function(t,n){return e.call(t,n)}},function(t,n,e){var r=e(49),o=e(15);t.exports=function(t){return r(o(t))}},function(t,n,e){t.exports=!e(8)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,e){var r=e(6),o=e(12);t.exports=e(4)?function(t,n,e){return r.f(t,n,o(1,e))}:function(t,n,e){return t[n]=e,t}},function(t,n,e){var r=e(10),o=e(30),i=e(24),u=Object.defineProperty;n.f=e(4)?Object.defineProperty:function(t,n,e){if(r(t),n=i(n,!0),r(e),o)try{return u(t,n,e)}catch(t){}if("get"in e||"set"in e)throw TypeError("Accessors not supported!");return"value"in e&&(t[n]=e.value),t}},function(t,n,e){var r=e(22)("wks"),o=e(13),i=e(1).Symbol,u="function"==typeof i,f=t.exports=function(t){return r[t]||(r[t]=u&&i[t]||(u?i:o)("Symbol."+t))};f.store=r},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,e){var r=e(35),o=e(16);t.exports=Object.keys||function(t){return r(t,o)}},function(t,n,e){var r=e(11);t.exports=function(t){if(!r(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var e=0,r=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++e+r).toString(36))}},function(t,n){var e=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=e)},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,e){var r=e(6).f,o=e(2),i=e(7)("toStringTag");t.exports=function(t,n,e){t&&!o(t=e?t:t.prototype,i)&&r(t,i,{configurable:!0,value:n})}},function(t,n,e){var r=e(22)("keys"),o=e(13);t.exports=function(t){return r[t]||(r[t]=o(t))}},function(t,n,e){var r=e(1),o="__core-js_shared__",i=r[o]||(r[o]={});t.exports=function(t){return i[t]||(i[t]={})}},function(t,n){var e=Math.ceil,r=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?r:e)(t)}},function(t,n,e){var r=e(11);t.exports=function(t,n){if(!r(t))return t;var e,o;if(n&&"function"==typeof(e=t.toString)&&!r(o=e.call(t)))return o;if("function"==typeof(e=t.valueOf)&&!r(o=e.call(t)))return o;if(!n&&"function"==typeof(e=t.toString)&&!r(o=e.call(t)))return o;throw TypeError("Can't convert object to primitive value")}},function(t,n,e){var r=e(1),o=e(14),i=e(18),u=e(26),f=e(6).f;t.exports=function(t){var n=o.Symbol||(o.Symbol=i?{}:r.Symbol||{});"_"==t.charAt(0)||t in n||f(n,t,{value:u.f(t)})}},function(t,n,e){n.f=e(7)},function(t,n,e){var r=e(1),o=e(14),i=e(46),u=e(5),f="prototype",c=function(t,n,e){var a,s,l,p=t&c.F,d=t&c.G,v=t&c.S,y=t&c.P,h=t&c.B,b=t&c.W,x=d?o:o[n]||(o[n]={}),g=x[f],m=d?r:v?r[n]:(r[n]||{})[f];d&&(e=n);for(a in e)s=!p&&m&&void 0!==m[a],s&&a in x||(l=s?m[a]:e[a],x[a]=d&&"function"!=typeof m[a]?e[a]:h&&s?i(l,r):b&&m[a]==l?function(t){var n=function(n,e,r){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,e)}return new t(n,e,r)}return t.apply(this,arguments)};return n[f]=t[f],n}(l):y&&"function"==typeof l?i(Function.call,l):l,y&&((x.virtual||(x.virtual={}))[a]=l,t&c.R&&g&&!g[a]&&u(g,a,l)))};c.F=1,c.G=2,c.S=4,c.P=8,c.B=16,c.W=32,c.U=64,c.R=128,t.exports=c},function(t,n){var e={}.toString;t.exports=function(t){return e.call(t).slice(8,-1)}},function(t,n,e){var r=e(11),o=e(1).document,i=r(o)&&r(o.createElement);t.exports=function(t){return i?o.createElement(t):{}}},function(t,n,e){t.exports=!e(4)&&!e(8)(function(){return 7!=Object.defineProperty(e(29)("div"),"a",{get:function(){return 7}}).a})},function(t,n,e){"use strict";var r=e(18),o=e(27),i=e(36),u=e(5),f=e(2),c=e(17),a=e(51),s=e(20),l=e(58),p=e(7)("iterator"),d=!([].keys&&"next"in[].keys()),v="@@iterator",y="keys",h="values",b=function(){return this};t.exports=function(t,n,e,x,g,m,w){a(e,n,x);var O,S,_,j=function(t){if(!d&&t in A)return A[t];switch(t){case y:return function(){return new e(this,t)};case h:return function(){return new e(this,t)}}return function(){return new e(this,t)}},P=n+" Iterator",E=g==h,M=!1,A=t.prototype,T=A[p]||A[v]||g&&A[g],L=T||j(g),N=g?E?j("entries"):L:void 0,C="Array"==n?A.entries||T:T;if(C&&(_=l(C.call(new t)),_!==Object.prototype&&(s(_,P,!0),r||f(_,p)||u(_,p,b))),E&&T&&T.name!==h&&(M=!0,L=function(){return T.call(this)}),r&&!w||!d&&!M&&A[p]||u(A,p,L),c[n]=L,c[P]=b,g)if(O={values:E?L:j(h),keys:m?L:j(y),entries:N},w)for(S in O)S in A||i(A,S,O[S]);else o(o.P+o.F*(d||M),n,O);return O}},function(t,n,e){var r=e(10),o=e(55),i=e(16),u=e(21)("IE_PROTO"),f=function(){},c="prototype",a=function(){var t,n=e(29)("iframe"),r=i.length,o="<",u=">";for(n.style.display="none",e(48).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write(o+"script"+u+"document.F=Object"+o+"/script"+u),t.close(),a=t.F;r--;)delete a[c][i[r]];return a()};t.exports=Object.create||function(t,n){var e;return null!==t?(f[c]=r(t),e=new f,f[c]=null,e[u]=t):e=a(),void 0===n?e:o(e,n)}},function(t,n,e){var r=e(35),o=e(16).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return r(t,o)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,e){var r=e(2),o=e(3),i=e(45)(!1),u=e(21)("IE_PROTO");t.exports=function(t,n){var e,f=o(t),c=0,a=[];for(e in f)e!=u&&r(f,e)&&a.push(e);for(;n.length>c;)r(f,e=n[c++])&&(~i(a,e)||a.push(e));return a}},function(t,n,e){t.exports=e(5)},function(t,n,e){var r=e(15);t.exports=function(t){return Object(r(t))}},function(t,n,e){t.exports={default:e(41),__esModule:!0}},function(t,n,e){t.exports={default:e(42),__esModule:!0}},function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var o=e(39),i=r(o),u=e(38),f=r(u),c="function"==typeof f.default&&"symbol"==typeof i.default?function(t){return typeof t}:function(t){return t&&"function"==typeof f.default&&t.constructor===f.default&&t!==f.default.prototype?"symbol":typeof t};n.default="function"==typeof f.default&&"symbol"===c(i.default)?function(t){return"undefined"==typeof t?"undefined":c(t)}:function(t){return t&&"function"==typeof f.default&&t.constructor===f.default&&t!==f.default.prototype?"symbol":"undefined"==typeof t?"undefined":c(t)}},function(t,n,e){e(65),e(63),e(66),e(67),t.exports=e(14).Symbol},function(t,n,e){e(64),e(68),t.exports=e(26).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,e){var r=e(3),o=e(61),i=e(60);t.exports=function(t){return function(n,e,u){var f,c=r(n),a=o(c.length),s=i(u,a);if(t&&e!=e){for(;a>s;)if(f=c[s++],f!=f)return!0}else for(;a>s;s++)if((t||s in c)&&c[s]===e)return t||s||0;return!t&&-1}}},function(t,n,e){var r=e(43);t.exports=function(t,n,e){if(r(t),void 0===n)return t;switch(e){case 1:return function(e){return t.call(n,e)};case 2:return function(e,r){return t.call(n,e,r)};case 3:return function(e,r,o){return t.call(n,e,r,o)}}return function(){return t.apply(n,arguments)}}},function(t,n,e){var r=e(9),o=e(34),i=e(19);t.exports=function(t){var n=r(t),e=o.f;if(e)for(var u,f=e(t),c=i.f,a=0;f.length>a;)c.call(t,u=f[a++])&&n.push(u);return n}},function(t,n,e){t.exports=e(1).document&&document.documentElement},function(t,n,e){var r=e(28);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==r(t)?t.split(""):Object(t)}},function(t,n,e){var r=e(28);t.exports=Array.isArray||function(t){return"Array"==r(t)}},function(t,n,e){"use strict";var r=e(32),o=e(12),i=e(20),u={};e(5)(u,e(7)("iterator"),function(){return this}),t.exports=function(t,n,e){t.prototype=r(u,{next:o(1,e)}),i(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,e){var r=e(9),o=e(3);t.exports=function(t,n){for(var e,i=o(t),u=r(i),f=u.length,c=0;f>c;)if(i[e=u[c++]]===n)return e}},function(t,n,e){var r=e(13)("meta"),o=e(11),i=e(2),u=e(6).f,f=0,c=Object.isExtensible||function(){return!0},a=!e(8)(function(){return c(Object.preventExtensions({}))}),s=function(t){u(t,r,{value:{i:"O"+ ++f,w:{}}})},l=function(t,n){if(!o(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!i(t,r)){if(!c(t))return"F";if(!n)return"E";s(t)}return t[r].i},p=function(t,n){if(!i(t,r)){if(!c(t))return!0;if(!n)return!1;s(t)}return t[r].w},d=function(t){return a&&v.NEED&&c(t)&&!i(t,r)&&s(t),t},v=t.exports={KEY:r,NEED:!1,fastKey:l,getWeak:p,onFreeze:d}},function(t,n,e){var r=e(6),o=e(10),i=e(9);t.exports=e(4)?Object.defineProperties:function(t,n){o(t);for(var e,u=i(n),f=u.length,c=0;f>c;)r.f(t,e=u[c++],n[e]);return t}},function(t,n,e){var r=e(19),o=e(12),i=e(3),u=e(24),f=e(2),c=e(30),a=Object.getOwnPropertyDescriptor;n.f=e(4)?a:function(t,n){if(t=i(t),n=u(n,!0),c)try{return a(t,n)}catch(t){}if(f(t,n))return o(!r.f.call(t,n),t[n])}},function(t,n,e){var r=e(3),o=e(33).f,i={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],f=function(t){try{return o(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==i.call(t)?f(t):o(r(t))}},function(t,n,e){var r=e(2),o=e(37),i=e(21)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=o(t),r(t,i)?t[i]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,e){var r=e(23),o=e(15);t.exports=function(t){return function(n,e){var i,u,f=String(o(n)),c=r(e),a=f.length;return c<0||c>=a?t?"":void 0:(i=f.charCodeAt(c),i<55296||i>56319||c+1===a||(u=f.charCodeAt(c+1))<56320||u>57343?t?f.charAt(c):i:t?f.slice(c,c+2):(i-55296<<10)+(u-56320)+65536)}}},function(t,n,e){var r=e(23),o=Math.max,i=Math.min;t.exports=function(t,n){return t=r(t),t<0?o(t+n,0):i(t,n)}},function(t,n,e){var r=e(23),o=Math.min;t.exports=function(t){return t>0?o(r(t),9007199254740991):0}},function(t,n,e){"use strict";var r=e(44),o=e(52),i=e(17),u=e(3);t.exports=e(31)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,e=this._i++;return!t||e>=t.length?(this._t=void 0,o(1)):"keys"==n?o(0,e):"values"==n?o(0,t[e]):o(0,[e,t[e]])},"values"),i.Arguments=i.Array,r("keys"),r("values"),r("entries")},function(t,n){},function(t,n,e){"use strict";var r=e(59)(!0);e(31)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,e=this._i;return e>=n.length?{value:void 0,done:!0}:(t=r(n,e),this._i+=t.length,{value:t,done:!1})})},function(t,n,e){"use strict";var r=e(1),o=e(2),i=e(4),u=e(27),f=e(36),c=e(54).KEY,a=e(8),s=e(22),l=e(20),p=e(13),d=e(7),v=e(26),y=e(25),h=e(53),b=e(47),x=e(50),g=e(10),m=e(3),w=e(24),O=e(12),S=e(32),_=e(57),j=e(56),P=e(6),E=e(9),M=j.f,A=P.f,T=_.f,L=r.Symbol,N=r.JSON,C=N&&N.stringify,k="prototype",F=d("_hidden"),q=d("toPrimitive"),I={}.propertyIsEnumerable,B=s("symbol-registry"),D=s("symbols"),W=s("op-symbols"),H=Object[k],K="function"==typeof L,R=r.QObject,J=!R||!R[k]||!R[k].findChild,U=i&&a(function(){return 7!=S(A({},"a",{get:function(){return A(this,"a",{value:7}).a}})).a})?function(t,n,e){var r=M(H,n);r&&delete H[n],A(t,n,e),r&&t!==H&&A(H,n,r)}:A,G=function(t){var n=D[t]=S(L[k]);return n._k=t,n},$=K&&"symbol"==typeof L.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof L},z=function(t,n,e){return t===H&&z(W,n,e),g(t),n=w(n,!0),g(e),o(D,n)?(e.enumerable?(o(t,F)&&t[F][n]&&(t[F][n]=!1),e=S(e,{enumerable:O(0,!1)})):(o(t,F)||A(t,F,O(1,{})),t[F][n]=!0),U(t,n,e)):A(t,n,e)},Y=function(t,n){g(t);for(var e,r=b(n=m(n)),o=0,i=r.length;i>o;)z(t,e=r[o++],n[e]);return t},Q=function(t,n){return void 0===n?S(t):Y(S(t),n)},X=function(t){var n=I.call(this,t=w(t,!0));return!(this===H&&o(D,t)&&!o(W,t))&&(!(n||!o(this,t)||!o(D,t)||o(this,F)&&this[F][t])||n)},V=function(t,n){if(t=m(t),n=w(n,!0),t!==H||!o(D,n)||o(W,n)){var e=M(t,n);return!e||!o(D,n)||o(t,F)&&t[F][n]||(e.enumerable=!0),e}},Z=function(t){for(var n,e=T(m(t)),r=[],i=0;e.length>i;)o(D,n=e[i++])||n==F||n==c||r.push(n);return r},tt=function(t){for(var n,e=t===H,r=T(e?W:m(t)),i=[],u=0;r.length>u;)!o(D,n=r[u++])||e&&!o(H,n)||i.push(D[n]);return i};K||(L=function(){if(this instanceof L)throw TypeError("Symbol is not a constructor!");var t=p(arguments.length>0?arguments[0]:void 0),n=function(e){this===H&&n.call(W,e),o(this,F)&&o(this[F],t)&&(this[F][t]=!1),U(this,t,O(1,e))};return i&&J&&U(H,t,{configurable:!0,set:n}),G(t)},f(L[k],"toString",function(){return this._k}),j.f=V,P.f=z,e(33).f=_.f=Z,e(19).f=X,e(34).f=tt,i&&!e(18)&&f(H,"propertyIsEnumerable",X,!0),v.f=function(t){return G(d(t))}),u(u.G+u.W+u.F*!K,{Symbol:L});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),et=0;nt.length>et;)d(nt[et++]);for(var nt=E(d.store),et=0;nt.length>et;)y(nt[et++]);u(u.S+u.F*!K,"Symbol",{for:function(t){return o(B,t+="")?B[t]:B[t]=L(t)},keyFor:function(t){if($(t))return h(B,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){J=!0},useSimple:function(){J=!1}}),u(u.S+u.F*!K,"Object",{create:Q,defineProperty:z,defineProperties:Y,getOwnPropertyDescriptor:V,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),N&&u(u.S+u.F*(!K||a(function(){var t=L();return"[null]"!=C([t])||"{}"!=C({a:t})||"{}"!=C(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!$(t)){for(var n,e,r=[t],o=1;arguments.length>o;)r.push(arguments[o++]);return n=r[1],"function"==typeof n&&(e=n),!e&&x(n)||(n=function(t,n){if(e&&(n=e.call(this,t,n)),!$(n))return n}),r[1]=n,C.apply(N,r)}}}),L[k][q]||e(5)(L[k],q,L[k].valueOf),l(L,"Symbol"),l(Math,"Math",!0),l(r.JSON,"JSON",!0)},function(t,n,e){e(25)("asyncIterator")},function(t,n,e){e(25)("observable")},function(t,n,e){e(62);for(var r=e(1),o=e(5),i=e(17),u=e(7)("toStringTag"),f=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],c=0;c<5;c++){var a=f[c],s=r[a],l=s&&s.prototype;l&&!l[u]&&o(l,u,a),i[a]=i.Array}},function(t,n){"use strict";var e={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&t.indexOf("KHTML")==-1,mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:t.indexOf("Safari")==-1,weixin:t.indexOf("MicroMessenger")==-1}}()};t.exports=e},function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}var o=e(40),i=r(o),u=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):o[t]||t}function n(t){return l[t]}var e=/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,r=/['<> "&]/g,o={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},f=/\u00a0/g,c=/<br\s*\/?>/gi,a=/\r?\n/g,s=/\s/g,l={};for(var p in o)l[o[p]]=p;return o["&apos;"]="'",l["'"]="&#39;",{encode:function(t){return t?(""+t).replace(r,n).replace(a,"<br/>").replace(s,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(c,"\n").replace(e,t).replace(f," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],e=0,r=t.length;r>e;e++)n.push(t.charCodeAt(e).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],e=0,r=t.length;r>e;e++)n.push(t.charCodeAt(e).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],e=0,r=t.length;r>e;e+=2)n.push(String.fromCharCode("0x"+t.slice(e,e+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,e=t.length;e>n;n++)t[n]=u.encodeObject(t[n]);else if("object"==("undefined"==typeof t?"undefined":(0,i.default)(t)))for(var r in t)t[r]=u.encodeObject(t[r]);else if("string"==typeof t)return u.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=u},function(t,n){function e(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=e},function(t,n){function e(t,n){if(t.classList)t.classList.remove(n);else{var e=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(e," ")}}t.exports=e},,,function(t,n){"use strict";function e(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){var n=document.querySelectorAll(".article-entry a:not(.article-more-a)");n.forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}var e=document.querySelector("#js-aboutme");e&&0!==e.length&&(e.innerHTML=e.innerText)}t.exports={init:e}},,,,,,,,,function(t,n){function e(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var e=t.nextSibling;return e?t.parentNode.insertBefore(n,e):t.parentNode.appendChild(n)}t.exports=e}])</script><script src="/./main.f6a68c.js"></script><script>!function(){var e=function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)};e("/slider.885efe.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">无证程序员&lt;br&gt;&lt;br&gt;资深足球小将</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>